<!doctype html>
<html itemscope itemtype="http://schema.org/Event">
<head>
  <title itemprop="name">The 1st Workshop on Machine Learning and Systems (EuroMLSys)</title>

  <meta charset="utf-8">
  <meta name="author" content="The 1st Workshop on Machine Learning and Systems (EuroMLSys)" />
  <meta name="description" content="The 1st Workshop on Machine Learning and Systems (EuroMLSys)">
  <meta name="viewport" content="width=device-width">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <meta name="generator" content="Hugo 0.54.0" />
  <meta property="og:title" content="The 1st Workshop on Machine Learning and Systems (EuroMLSys)" />
<meta property="og:description" content="The 1st Workshop on Machine Learning and Systems (EuroMLSys)" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://euromlsys.github.io/" />

<meta property="og:image" content="https://euromlsys.github.io/img/euromlsys.png" />


  <meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://euromlsys.github.io/img/euromlsys.png"/>

<meta name="twitter:title" content="The 1st Workshop on Machine Learning and Systems (EuroMLSys)"/>
<meta name="twitter:description" content="The 1st Workshop on Machine Learning and Systems (EuroMLSys)"/>


  <link rel="shortcut icon" href="/img/favicon.ico" type="image/x-icon">
  <link rel="apple-touch-icon" href="/img/apple-touch-icon.png">
  <link rel="stylesheet" type="text/css" href="/css/main.css">
  <link rel="stylesheet" type="text/css" href="/css/euromlsys.css">

  <script
  src="https://code.jquery.com/jquery-1.12.4.js"
  integrity="sha256-Qw82+bXyGq6MydymqBxNPYTaUXXq7c8v3CwiYwLLNXU="
  crossorigin="anonymous"></script>
  <script type="text/javascript" src="/js/sections.js"></script>

</head>
<body>
  <div class="global">

    <nav id="nav">
  <ul class="wrapper">
    
      <li class="nav-item">
        <a href="#about" id="link-about" title="About" class="nav-link">
          About
        </a>
      </li>
    
      <li class="nav-item">
        <a href="#cfp" id="link-cfp" title="Call for Papers" class="nav-link">
          Call for Papers
        </a>
      </li>
    
      <li class="nav-item">
        <a href="#accepted-papers" id="link-accepted-papers" title="Accepted Papers" class="nav-link">
          Accepted Papers
        </a>
      </li>
    
      <li class="nav-item">
        <a href="#schedule" id="link-schedule" title="Program" class="nav-link">
          Program
        </a>
      </li>
    
      <li class="nav-item">
        <a href="#speakers" id="link-speakers" title="Keynotes" class="nav-link">
          Keynotes
        </a>
      </li>
    
      <li class="nav-item">
        <a href="#committees" id="link-committees" title="Committees" class="nav-link">
          Committees
        </a>
      </li>
    
      <li class="nav-item">
        <a href="#contact" id="link-contact" title="Contact" class="nav-link">
          Contact
        </a>
      </li>
    
  </ul>
</nav>

<hr>

    

<header class="header">
  <div class="wrapper">
    <h1 class="logo-name">
      <a class="logo-link" href="#" title="The 1st Workshop on Machine Learning and Systems (EuroMLSys)" itemprop="name">The 1st Workshop on Machine Learning and Systems (EuroMLSys)</a>
    </h1>
    <h2 class="subtitle">co-located with <a href="https://2021.eurosys.org">EuroSys '21</a></h2>
    <h2 class="tagline">April 26th 2021, Virtually in Edinburgh, Scotland, UK,</h2>

    <div class="call-action-area">
      

      
    </div>

    <div>
      <img src="/img/euromlsys-white.png">
    </div>
  </div>
</header>

<hr>


    <div class="content" id="content">
      <div class="wrapper">
        
          <section class="about section" id="about">
            <p itemprop="description">
    The recent wave of research focusing on machine intelligence (machine learning and artificial intelligence) and its
    applications has been fuelled by both hardware improvements and deep learning frameworks that simplify the design
    and training of neural models. Advances in AI also accelerate research towards Reinforcement Learning (RL), where
    dynamic control mechanisms are designed to tackle complex tasks. Further, machine learning based optimisation, such
    as Bayesian Optimisation, is gaining traction in the computer systems community where optimisation needs to scale
    with complex and large parameter spaces; areas of interest range from hyperparameter tuning to system configuration
    tuning,
</p>
<p itemprop="description">
    The EuroMLSys workshop will provide a platform for discussing emerging trends in building frameworks, programming
    models, optimisation algorithms, and software engineering tools to support AI/ML applications. At the same time,
    using ML for building such frameworks or optimisation tools will be discussed. EuroMLSys aims to bridge the gap
    between AI research and practice, through a technical program of fresh ideas on software infrastructure, tools,
    design principles, and theory/algorithms (including issues of instability, data efficiency, etc.), from a systems
    perspective. We will also explore potential applications that will take advantage of ML.
</p>

<h4>Registration</h4>
<p>The registration for EuroMLSys'21 and EuroSys'21 is free of charge.<br>
<b></b>Register via this <a href="https://www.eventbrite.co.uk/e/eurosys21-tickets-141026430851">[Link]</a>.
</p>

          </section>
        
          <section class="cfp section" id="cfp">
            <h2 class="section-title">Call for Papers</h2>

<p>The EuroMLSys workshop focuses on research topics at the intersection of Machine Learning and Computer Systems, and
    it is the first workshop to be co-located with EuroSys that addresses this emerging topic.
</p>
<p>Topics of interest include, but are not limited to, the following:</p>
    <ul class="list">
        <li>Scheduling algorithms for data processing clusters</li>
        <li>Custom hardware for machine learning</li>
        <li> Programming languages for machine learning</li>
        <li>Benchmarking systems (for machine learning algorithms)</li>
        <li>Synthetic input data generation for training</li>
        <li> Systems for training and serving machine learning models at scale</li>
        <li>Graph neural networks</li>
        <li>Neural network compression and pruning in systems</li>
        <li> Systems for incremental learning algorithms</li>
        <li> Large scale distributed learning algorithms in practice</li>
        <li>Database systems for large scale learning</li>
        <li>Model understanding tools (debugging, visualisation, etc.)</li>
        <li>Systems for model-free and model-based Reinforcement Learning
        </li>
        <li>Optimisation in end-to-end deep learning</li>
        <li> System optimisation using Bayesian Optimisation</li>
        <li> Acceleration of model building (e.g.,
            imitation learning in RL)</li>
        <li> Use of probabilistic models in ML/AI
            application</li>
        <li> Learning models for inferring
            network attacks,
            device/service fingerprinting,
            congestion, etc.</li>
        <li> Techniques to collect and
            analyze network data in a
            privacy-preserving manner</li>
        <li> Learning models to capture
            network events and
            control actions</li>
        <li>Machine learning in
            networking (e.g., use of
            Deep RL in networking)</li>
        <li>Analysis of distributed ML algorithms</li>
        <li>Semantics for distributed ML languages</li>
        <li>Probabilistic modelling for distributed ML algorithms</li>
        <li>Synchronisation and state control of distributed ML algorithms</li>
    </ul>
    <p></p>
<p>Accepted papers will be published in the ACM Digital Library (you can opt out from this).</p>

          </section>
        
          <section class="accepted-papers section" id="accepted-papers">
            
<h4>Accepted Papers</h4>
<h5>Oral Presentation</h5>
<p>
<ul class="list">
  <li>
    <b>"Learned Low Precision Graph Neural Networks"</b> —
    <i>Yiren Zhao, Duo Wang, Daniel Bates, Robert Mullins, Mateja Jamnik, and Pietro Lio (The University of Cambridge)</i>
  </li>
  <br>
  <li>
    <b>"Optimizing Inference Performance of Transformers on CPUs"</b> —
    <i>Dave Dice and Alex Kogan (Oracle Labs)</i>
  </li>
  <br>
  <li>
    <b>"DistIR: An Intermediate Representation for Optimizing Distributed Neural Networks"</b> —
    <i>Keshav Santhanam (Stanford University); Siddharth Krishna, Ryota Tomioka, Andrew Fitzgibbon, and Tim Harris (Microsoft)</i>
  </li>
  <br>
  <li>
    <b>"Predicting CPU Usage for Proactive Autoscaling"</b> —
    <i>Thomas Wang and Simone Ferlin (Ericsson AB); Marco Chiesa (KTH Royal Institute of Technology)</i>
  </li>
  <br>
  <li>
    <b>"Are we there yet? Estimating Training Time for Recommendation Systems"</b> —
    <i>Iulia Paun (University of Glasgow); Yashar Moshfeghi (University of Strathclyde); Nikos Ntarmos (University of Glasgow)</i>
  </li>
  <br>
  <li>
    <b>"Vate: Runtime Adaptable Probabilistic Programming for Java  "</b> —
    <i>Daniel Goodman, Adam Pocock, Jason Peck, and Guy Steele (Oracle Labs)</i>
  </li>
  <br>
  <li>
    <b>"μNAS: Constrained Neural Architecture Search for Microcontrollers"</b> —
    <i>Edgar Liberis (University of Cambridge); Łukasz Dudziak (Samsung AI Center Cambridge); Nicholas D. Lane (University of Cambridge and Samsung AI)</i>
  </li>
  <br>
  <li>
    <b>"Interference-Aware Scheduling for Inference Serving"</b> —
    <i>Daniel Mendoza, Francisco Romero, Qian Li, Neeraja J. Yadwadkar, and Christos Kozyrakis (Stanford University)</i>
  </li>
  <br>
  <li>
    <b>"DISC: A Dynamic Shape Compiler for Machine Learning Workloads"</b> —
    <i>Kai Zhu, Wenyi Zhao, Zhen Zheng, Tianyou Guo, Pengzhan Zhao, Junjie Bai, Jun Yang, Xiaoyong Liu, Lansong Diao, and Wei Lin (Alibaba Group)</i>
  </li>
  <br>
  <li>
    <b>"Towards Mitigating Device Heterogeneity in Federated Learning via Adaptive Model Quantization"</b> —
    <i>Ahmed M. Abdelmoniem and Marco Canini (KAUST)</i>
  </li>
  <br>
  <li>
    <b>"High-Dimensional Bayesian Optimization with Multi-Task Learning for RocksDB"</b> —
    <i>Sami Alabed and Eiko Yoneki (University of Cambridge)</i>
  </li>
  <br>
  <li>
    <b>"Developing a Siamese Network for Intrusion Detection Systems"</b> —
    <i>Hanan Hindy (Division of Cyber Security, Abertay University Dundee, Scotland, UK); Christos Tachtatzis and Robert Atkinson (EEE Department, University of Strathclyde, Glasgow, Scotland, UK); Ethan Bayne (Division of Cyber Security, Abertay University Dundee, Scotland, UK); Xavier Bellekens (EEE Department, University of Strathclyde, Glasgow, Scotland, UK)</i>
  </li>
  <p></p>
  <h5>Poster Presentation</h5>
  <li>
    <b>"DPD-InfoGAN: Differentially Private Distributed InfoGAN"</b> —
    <i>Vaikkunth Mugunthan (Massachusetts Institute of Technology); Vignesh Gokul (UCSD); Lalana Kagal (Massachusetts Institute of Technology); Shlomo Dubnov (UCSD)</i>
  </li>
  <br>
  <li>
    <b>"Towards Optimal Configuration of Microservices"</b> —
    <i>Gagan Somashekar and Anshul Gandhi (Stony Brook University)</i>
  </li>
  <br>
  <li>
    <b>"Towards a General Framework for ML-based Self-tuning Databases"</b> —
    <i>Thomas Schmied, Diego Didona, Andreas Doering, Thomas Parnell, and Nikolas Ioannou (IBM Research - Zurich)</i>
  </li>
  <br>
  <li>
    <b>"Queen Jane Approximately: Enabling Efficient Neural Network Inference with Context-Adaptivity"</b> —
    <i>Octavian Machidon, Davor Sluga, and Veljko Pejović (Faculty of Computer and Information Science, University of Ljubljana, Slovenia)</i>
  </li>
  <br>
  <li>
    <b>"AutoAblation: Automated Parallel Ablation Studies for Deep Learning"</b> —
    <i>Sina Sheikholeslami (KTH Royal Institute of Technology); Moritz Meister (Logical Clocks AB); Tianze Wang and Amir H. Payberah (KTH Royal Institute of Technology); Vladimir Vlassov (KTH Royal Institute of Techonology); Jim Dowling (KTH Royal Institute of Technology, Logical Clocks AB)</i>
  </li>
  <br>
  <li>
    <b>"Fast Optimisation of Convolutional Neural Network Inference using System Performance Models"</b> —
    <i>Rik Mulder (University of Edinburgh); Valentin Radu (University of Sheffield); Christophe Dubach (McGill University)</i>
  </li>
  <br>
</ul>
</p>

          </section>
        
          <section class="schedule section" id="schedule">
            <h2 class="section-title">Program</h2>

<div class="schedule-tbl">
  <table>
    <thead>
      <tr>
        <th class="schedule-time">Time</th>
        <th class="schedule-slot">Slot</th>
      </tr>
    </thead>
    <tbody>
      
        
          <tr class="schedule-other">
            <td class="schedule-time">15h00</td>
            <td class="schedule-slot">Introduction</td>
          </tr>
        
      
        
          <tr>
            <td class="schedule-time">15h10</td>
            <td class="schedule-slot">
            
            
              Keynote 1: Zhihao Jia
            
              <span class="speakers-company">CMU &amp; Facebook</span>
              <span class="schedule-description program">As an increasingly important workload, machine learning (ML) applications require different performance optimization techniques from traditional runtimes and compilers. In particular, to accelerate ML applications, it is generally necessary to perform ML computations on heterogeneous hardware and parallelize computations using multiple data dimensions, neither of which is even expressible in traditional compilers and runtimes. In this talk, I will present our recent work on automated discovery of performance optimizations to accelerate ML computations. TASO, the Tensor Algebra SuperOptimizer, optimizes the computation graphs of deep neural networks (DNNs) by automatically generating potential graph optimizations and formally verifying their correctness. TASO outperforms rule-based graph optimizers in existing ML systems (e.g., TensorFlow, TensorRT, and TVM) by up to 3x by automatically discovering novel graph optimizations, while also requiring significantly less human effort. FlexFlow is a system for accelerating distributed DNN training. FlexFlow identifies parallelization dimensions not considered in existing ML systems (e.g., TensorFlow and PyTorch) and automatically discovers fast parallelization strategies for a specific parallel machine. Companies and national labs are using FlexFlow to train production ML models that do not scale well in current ML systems, achieving over 10x performance improvement.</span>
            </td>
          </tr>
        
      
        
          <tr class="schedule-session">
            <td class="schedule-time">15h50</td>
            <td class="schedule-slot">Session 1: Systems, Compiler and PPL</td>
          </tr>
        
      
        
          <tr>
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              DISC: A Dynamic Shape Compiler for Machine Learning Workloads
            
              <span class="speakers-company">Kai Zhu, Wenyi Zhao, Zhen Zheng, Tianyou Guo, Pengzhan Zhao, Junjie Bai, Jun Yang, Xiaoyong Liu, Lansong Diao, Wei Lin (Alibaba Group)</span>
              <span class="schedule-description program"></span>
            </td>
          </tr>
        
      
        
          <tr>
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              High-Dimensional Bayesian Optimization with Multi-Task Learning for RocksDB 
            
              <span class="speakers-company">Sami Alabed, Eiko Yoneki (University of Cambridge)</span>
              <span class="schedule-description program"></span>
            </td>
          </tr>
        
      
        
          <tr>
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              Vate: Runtime Adaptable Probabilistic Programming for Java
            
              <span class="speakers-company">Daniel Goodman, Adam Pocock, Jason Peck, Guy Steele (Oracle Labs)</span>
              <span class="schedule-description program"></span>
            </td>
          </tr>
        
      
        
          <tr>
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              DistIR: An Intermediate Representation for Optimizing Distributed Neural Network
            
              <span class="speakers-company">Keshav Santhanam (Stanford University); Siddharth Krishna, Ryota Tomioka, Andrew Fitzgibbon, Tim Harris (Microsoft)</span>
              <span class="schedule-description program"></span>
            </td>
          </tr>
        
      
        
          <tr class="schedule-other">
            <td class="schedule-time">16h55</td>
            <td class="schedule-slot">Break</td>
          </tr>
        
      
        
          <tr class="schedule-session">
            <td class="schedule-time">17h05</td>
            <td class="schedule-slot">Session 2: Model Optimisation and NAS</td>
          </tr>
        
      
        
          <tr>
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              Optimizing Inference Performance of Transformers on CPUs 
            
              <span class="speakers-company">Dave Dice, Alex Kogan (Oracle Labs)</span>
              <span class="schedule-description program"></span>
            </td>
          </tr>
        
      
        
          <tr>
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              Learned Low Precision Graph Neural Networks
            
              <span class="speakers-company">Yiren Zhao, Duo Wang, Daniel Bates, Robert Mullins, Mateja Jamnik, Pietro Lio (The University of Cambridge)</span>
              <span class="schedule-description program"></span>
            </td>
          </tr>
        
      
        
          <tr>
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              μNAS: Constrained Neural Architecture Search for Microcontrollers
            
              <span class="speakers-company">Edgar Liberis (University of Cambridge); Łukasz Dudziak (Samsung AI Center Cambridge); Nicholas D. Lane (University of Cambridge and Samsung AI)</span>
              <span class="schedule-description program"></span>
            </td>
          </tr>
        
      
        
          <tr>
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              Towards Mitigating Device Heterogeneity in Federated Learning via Adaptive Model Quantization
            
              <span class="speakers-company">Ahmed M. Abdelmoniem, Marco Canini (KAUST)</span>
              <span class="schedule-description program"></span>
            </td>
          </tr>
        
      
        
          <tr>
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              DISC: A Dynamic Shape Compiler for Machine Learning Workloads
            
              <span class="speakers-company">Kai Zhu, Wenyi Zhao, Zhen Zheng, Tianyou Guo, Pengzhan Zhao, Junjie Bai, Jun Yang, Xiaoyong Liu, Lansong Diao, Wei Lin (Alibaba Group)</span>
              <span class="schedule-description program"></span>
            </td>
          </tr>
        
      
        
          <tr>
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              DISC: A Dynamic Shape Compiler for Machine Learning Workloads
            
              <span class="speakers-company">Kai Zhu, Wenyi Zhao, Zhen Zheng, Tianyou Guo, Pengzhan Zhao, Junjie Bai, Jun Yang, Xiaoyong Liu, Lansong Diao, Wei Lin (Alibaba Group)</span>
              <span class="schedule-description program"></span>
            </td>
          </tr>
        
      
        
          <tr class="schedule-other">
            <td class="schedule-time">18h10</td>
            <td class="schedule-slot">Break</td>
          </tr>
        
      
        
          <tr>
            <td class="schedule-time">18h20</td>
            <td class="schedule-slot">
            
            
              Keynote 2: Anna Goldie
            
              <span class="speakers-company">Google Brain &amp; Stanford University</span>
              <span class="schedule-description program"></span>
            </td>
          </tr>
        
      
        
          <tr class="schedule-other">
            <td class="schedule-time">19h00</td>
            <td class="schedule-slot">Break</td>
          </tr>
        
      
        
          <tr class="schedule-session">
            <td class="schedule-time">19h05</td>
            <td class="schedule-slot">Session 3: Scheduling, Training and Prediction</td>
          </tr>
        
      
        
          <tr>
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              Are we there yet? Estimating Training Time for Recommendation Systems
            
              <span class="speakers-company">Iulia Paun (University of Glasgow); Yashar Moshfeghi (University of Strathclyde); Nikos Ntarmos (University of Glasgow)</span>
              <span class="schedule-description program"></span>
            </td>
          </tr>
        
      
        
          <tr>
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              Interference-Aware Scheduling for Inference Serving 
            
              <span class="speakers-company">Daniel Mendoza, Francisco Romero, Qian Li, Neeraja J. Yadwadkar, Christos Kozyrakis (Stanford University)</span>
              <span class="schedule-description program"></span>
            </td>
          </tr>
        
      
        
          <tr>
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              Developing a Siamese Network for Intrusion Detection Systems
            
              <span class="speakers-company">Hanan Hindy (Division of Cyber Security, Abertay University Dundee, Scotland, UK); Christos Tachtatzis, Robert Atkinson (EEE Department, University of Strathclyde, Glasgow, Scotland, UK); Ethan Bayne (Division of Cyber Security, Abertay University Dundee, Scotland, UK); Xavier Bellekens (EEE Department, University of Strathclyde, Glasgow, Scotland, UK)</span>
              <span class="schedule-description program"></span>
            </td>
          </tr>
        
      
        
          <tr>
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              Predicting CPU Usage for Proactive Autoscaling 
            
              <span class="speakers-company">Thomas Wang, Simone Ferlin (Ericsson AB); Marco Chiesa (KTH Royal Institute of Technology)</span>
              <span class="schedule-description program"></span>
            </td>
          </tr>
        
      
        
          <tr class="schedule-other">
            <td class="schedule-time">20h10</td>
            <td class="schedule-slot">Break</td>
          </tr>
        
      
        
          <tr class="schedule-other">
            <td class="schedule-time">20h15</td>
            <td class="schedule-slot">Poster Session</td>
          </tr>
        
      
        
          <tr>
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              Queen Jane Approximately: Enabling Efficient Neural Network Inference with Context-Adaptivity
            
              <span class="speakers-company">Octavian Machidon, Davor Sluga, Veljko Pejović (Faculty of Computer and Information Science, University of Ljubljana, Slovenia)</span>
              <span class="schedule-description program"></span>
            </td>
          </tr>
        
      
        
          <tr>
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              DPD-InfoGAN: Differentially Private Distributed InfoGAN
            
              <span class="speakers-company">Vaikkunth Mugunthan (Massachusetts Institute of Technology); Vignesh Gokul (UCSD); Lalana Kagal (Massachusetts Institute of Technology); Shlomo Dubnov (UCSD)</span>
              <span class="schedule-description program"></span>
            </td>
          </tr>
        
      
        
          <tr>
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              Towards Optimal Configuration of Microservices
            
              <span class="speakers-company">Gagan Somashekar, Anshul Gandhi (Stony Brook University)</span>
              <span class="schedule-description program"></span>
            </td>
          </tr>
        
      
        
          <tr>
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              AutoAblation: Automated Parallel Ablation Studies for Deep Learning
            
              <span class="speakers-company">Sina Sheikholeslami (KTH Royal Institute of Technology); Moritz Meister (Logical Clocks AB); Tianze Wang, Amir H. Payberah (KTH Royal Institute of Technology); Vladimir Vlassov (KTH Royal Institute of Techonology); Jim Dowling (KTH Royal Institute of Technology, Logical Clocks AB)</span>
              <span class="schedule-description program"></span>
            </td>
          </tr>
        
      
        
          <tr>
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              Towards a General Framework for ML-based Self-tuning Databases
            
              <span class="speakers-company">Thomas Schmied, Diego Didona, Andreas Doering, Thomas Parnell, Nikolas Ioannou (IBM Research - Zurich)</span>
              <span class="schedule-description program"></span>
            </td>
          </tr>
        
      
        
          <tr>
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              Fast Optimisation of Convolutional Neural Network Inference using System Performance Models
            
              <span class="speakers-company">Rik Mulder (University of Edinburgh); Valentin Radu (University of Sheffield); Christophe Dubach (McGill University)</span>
              <span class="schedule-description program"></span>
            </td>
          </tr>
        
      
        
          <tr class="schedule-other">
            <td class="schedule-time">20h50</td>
            <td class="schedule-slot">Wrapup</td>
          </tr>
        
      
    </tbody>
  </table>
</div>

          </section>
        
          <section class="speakers section" id="speakers">
            <h2 class="section-title">Keynotes</h2>
<p> </p>
<ul class="speakers-list">

  <li class="speakers-item" itemprop="performer" itemscope itemtype="http://schema.org/Person">
    
      <span class="speaker-photo">
        <img class="photo" src="/img/zhihao.jpg" alt="Zhihao Jia" itemprop="image">
      </span>
    

    <h3 class="speech-title">
      
      <span class="speech-time">15h10</span>
      
      <span>Zhihao Jia</span>
      <span class="speakers-company">CMU &amp; Facebook</span>
    </h3>

    <h3 class="speakers-name">Automated Discovery of Machine Learning Optimizations
      
    </h3>
    <p class="schedule-description">As an increasingly important workload, machine learning (ML) applications require different performance optimization techniques from traditional runtimes and compilers. In particular, to accelerate ML applications, it is generally necessary to perform ML computations on heterogeneous hardware and parallelize computations using multiple data dimensions, neither of which is even expressible in traditional compilers and runtimes. In this talk, I will present our recent work on automated discovery of performance optimizations to accelerate ML computations. TASO, the Tensor Algebra SuperOptimizer, optimizes the computation graphs of deep neural networks (DNNs) by automatically generating potential graph optimizations and formally verifying their correctness. TASO outperforms rule-based graph optimizers in existing ML systems (e.g., TensorFlow, TensorRT, and TVM) by up to 3x by automatically discovering novel graph optimizations, while also requiring significantly less human effort. FlexFlow is a system for accelerating distributed DNN training. FlexFlow identifies parallelization dimensions not considered in existing ML systems (e.g., TensorFlow and PyTorch) and automatically discovers fast parallelization strategies for a specific parallel machine. Companies and national labs are using FlexFlow to train production ML models that do not scale well in current ML systems, achieving over 10x performance improvement.</p>
    <p class="speakers-bio">Bio: Zhihao Jia is currently a research scientist at Facebook and will join CMU as an assistant professor of computer science in Fall 2021. He obtained his Ph.D. at Stanford working with Alex Aiken and Matei Zaharia. His research interests lie in the intersection of computer systems and machine learning, with a focus on building efficient, scalable, and high-performance systems for ML computations.</p>
  </li>

  <li class="speakers-item" itemprop="performer" itemscope itemtype="http://schema.org/Person">
    
      <span class="speaker-photo">
        <img class="photo" src="/img/anna.jpg" alt="Anna Goldie" itemprop="image">
      </span>
    

    <h3 class="speech-title">
      
      <span class="speech-time">18h20</span>
      
      <span>Anna Goldie</span>
      <span class="speakers-company">Google Brain &amp; Stanford University</span>
    </h3>

    <h3 class="speakers-name">Details comming soon
      
    </h3>
    <p class="schedule-description">...</p>
    <p class="speakers-bio"></p>
  </li>

</ul>

          </section>
        
          <section class="committees section" id="committees">
            <h2 class="section-title">Committees</h2>

<h4>Workshop and TPC Chairs</h4>
<p>
<ul class="list">
    <li>Eiko Yoneki, University of Cambridge</li>
    <li>Paul Patras, University of Edinburgh</li>
</ul>
</p>
<p>
<h4>Technical Program Committee</h4>
<p>
    <ul class="list">
        <li>Sam Ainsworth, University of Edinburgh</li>
        <li>Sami Alabed, University of Cambridge</li>
        <li>Laurent Bindschaedler, MIT</li>
        <li>Jose Cano, University of Glasgow</li>
        <li>Jon Crowcroft, University of Cambridge</li>
        <li>Daniel Goodman, Oracle</li>
        <li>Hamed Haddadi, Imperial College London</li>
        <li>Zhihao Jia, CMU</li>
        <li>Alexandros Koliousis, NCH</li>
        <li>Dawei Li, Amazon</li>
        <li>Luisi Nardi, Stanford University/Lund University</li>
        <li>Amir Payberah, KTH</li>
        <li>Peter Pietzuch, Imperial College London</li>
        <li>Valentin Radu, University of Sheffield</li>
        <li>Amitabha Roy, Google</li>
        <li>Adam Ścibior, UBC</li>
        <li>Ryota Tomioka, MSR Cambridge</li>
        <li>Peter Triantafillou, University of Warwick</li>
        <li>Aaron Zhao, University of Cambridge</li>

    </ul>
</p>
</p>

<h4>Web Chair</h4>
<ul class="list">
    <li>Alexis Duque, University of Edinburgh</li>
</ul>

          </section>
        
          <section class="contact section" id="contact">
            <h2 class="section-title">Contact</h2>
<div>
  <p> For any question(s) related to EuroMLSys 2021, please contact us: <a href="mailto:organizers-2021@eurosys.org">organizers-2021@eurosys.org</a>
  <p><img src="/img/twitter.png" height="50px">Follow us on Twitter: <a href="https://twitter.com/euromlsys">@euromlsys</a>

</div>
          </section>
        

        <footer class="footer">
          <p>Sponsored by
            <a href="https://research.fb.com/" class="sponsor" itemprop="url" target="_blank">
              <img src="img/fb.svg" width="200px" alt="Facebook Research" class="photo" itemprop="image">
            </a>
          </p>
          <p>Made with ♥ by EuroMLSys'21 team :).</p>
        </footer>
      </div>
    </div>
  </div>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="/js/jquery.js"><\/script>')</script>
  
  
</body>
</html>
