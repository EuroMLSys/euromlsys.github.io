<!doctype html>
<html itemscope itemtype="http://schema.org/Event">
<head>
  <title itemprop="name">The 2nd Workshop on Machine Learning and Systems (EuroMLSys)</title>

  <meta charset="utf-8">
  <meta name="author" content="The 2nd Workshop on Machine Learning and Systems (EuroMLSys)" />
  <meta name="description" content="The 2nd Workshop on Machine Learning and Systems (EuroMLSys)">
  <meta name="viewport" content="width=device-width">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <meta name="generator" content="Hugo 0.54.0" />
  <meta property="og:title" content="The 2nd Workshop on Machine Learning and Systems (EuroMLSys)" />
<meta property="og:description" content="The 2nd Workshop on Machine Learning and Systems (EuroMLSys)" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://euromlsys.github.io/" />

<meta property="og:image" content="https://euromlsys.github.io/img/euromlsys.png" />


  <meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://euromlsys.github.io/img/euromlsys.png"/>

<meta name="twitter:title" content="The 2nd Workshop on Machine Learning and Systems (EuroMLSys)"/>
<meta name="twitter:description" content="The 2nd Workshop on Machine Learning and Systems (EuroMLSys)"/>


  <link rel="shortcut icon" href="/img/favicon.ico" type="image/x-icon">
  <link rel="apple-touch-icon" href="/img/apple-touch-icon.png">
  <link rel="stylesheet" type="text/css" href="/css/main.css">
  <link rel="stylesheet" type="text/css" href="/css/euromlsys.css">

  <script
  src="https://code.jquery.com/jquery-1.12.4.js"
  integrity="sha256-Qw82+bXyGq6MydymqBxNPYTaUXXq7c8v3CwiYwLLNXU="
  crossorigin="anonymous"></script>
  <script type="text/javascript" src="/js/sections.js"></script>

</head>
<body>
  <div class="global">

    <nav id="nav">
  <ul class="wrapper">
    
      <li class="nav-item">
        <a href="#about" id="link-about" title="Home" class="nav-link">
          Home
        </a>
      </li>
    
      <li class="nav-item">
        <a href="#cfp" id="link-cfp" title="Call for Papers" class="nav-link">
          Call for Papers
        </a>
      </li>
    
      <li class="nav-item">
        <a href="#accepted-papers" id="link-accepted-papers" title="Accepted Papers" class="nav-link">
          Accepted Papers
        </a>
      </li>
    
      <li class="nav-item">
        <a href="#schedule" id="link-schedule" title="Program" class="nav-link">
          Program
        </a>
      </li>
    
      <li class="nav-item">
        <a href="#sponsors" id="link-sponsors" title="Sponsors" class="nav-link">
          Sponsors
        </a>
      </li>
    
      <li class="nav-item">
        <a href="#committees" id="link-committees" title="Committees" class="nav-link">
          Committees
        </a>
      </li>
    
      <li class="nav-item">
        <a href="#contact" id="link-contact" title="Contact" class="nav-link">
          Contact
        </a>
      </li>
    
  </ul>
</nav>

<hr>

    

<header class="header">
  <div class="wrapper">
    <h1 class="logo-name">
      <a class="logo-link" href="#" title="The 2nd Workshop on Machine Learning and Systems (EuroMLSys)" itemprop="name">The 2nd Workshop on Machine Learning and Systems (EuroMLSys)</a>
    </h1>
    <h2 class="subtitle">co-located with <a href="https://2022.eurosys.org">EuroSys '22</a></h2>
    <h2 class="tagline">April 5th 2022, Rennes, France</h2>

    <div class="call-action-area">
      

      
    </div>

    <div>
      <img src="/img/euromlsys-white.png">
    </div>
  </div>
</header>

<hr>


    <div class="content" id="content">
      <div class="wrapper">
        
          <section class="about section" id="about">
            <p itemprop="description">
    The recent wave of research focusing on machine intelligence (machine learning and artificial intelligence) and its
    applications has been fuelled by both hardware improvements and deep learning frameworks that simplify the design
    and training of neural models. Advances in AI also accelerate research towards Reinforcement Learning (RL), where
    dynamic control mechanisms are designed to tackle complex tasks. Further, machine learning based optimisation, such
    as Bayesian Optimisation, is gaining traction in the computer systems community where optimisation needs to scale
    with complex and large parameter spaces; areas of interest range from hyperparameter tuning to system configuration
    tuning,
</p>
<p itemprop="description">
    The EuroMLSys workshop will provide a platform for discussing emerging trends in building frameworks, programming
    models, optimisation algorithms, and software engineering tools to support AI/ML applications. At the same time,
    using ML for building such frameworks or optimisation tools will be discussed. EuroMLSys aims to bridge the gap
    between AI research and practice, through a technical program of fresh ideas on software infrastructure, tools,
    design principles, and theory/algorithms (including issues of instability, data efficiency, etc.), from a systems
    perspective. We will also explore potential applications that will take advantage of ML.
</p>

<h4>Key dates</h4>
<p>
<ul class="list">
    <li><b>Paper submission deadline (hard):</b> <s>February 12, 2022</s>  <b>February 17, 2022 (23:59 AoE, UTC-12)</b></li>
    <li><b>Acceptance notification:</b> March 11, 2022</li>
    <li><b>Final paper due:</b> <s>March 18, 2022</s>  <b>March 21, 2022</b></li>
    <li><b>Workshop:</b> April 5, 2022 (full-day workshop)</li>
</ul>
</p>
</p>

<h4>Registration</h4>

<p>
Register via this <a href="https://2022.eurosys.org/registration/">[Link]</a>.
</p>

<h4>Past Editions</h4>
<p>
<ul class="list">
    <li><a href="https://2021.euromlsys.eu">EuroMLSys 2021</a></li>
</ul>
</p>
</p>

          </section>
        
          <section class="cfp section" id="cfp">
            <h2 class="section-title">Call for Papers</h2>

<p>EuroMLSys is an interdisciplinary workshop that brings together researchers in computer architecture, systems and machine learning, along with practitioners who are active in these emerging areas.
</p>
<p>Topics of interest include, but are not limited to, the following:</p>
    <ul class="list">
        <li>Scheduling algorithms for data processing clusters</li>
        <li>Custom hardware for machine learning</li>
        <li> Programming languages for machine learning</li>
        <li>Benchmarking systems (for machine learning algorithms)</li>
        <li>Synthetic input data generation for training</li>
        <li> Systems for training and serving machine learning models at scale</li>
        <li>Graph neural networks</li>
        <li>Neural network compression and pruning in systems</li>
        <li> Systems for incremental learning algorithms</li>
        <li> Large scale distributed learning algorithms in practice</li>
        <li>Database systems for large scale learning</li>
        <li>Model understanding tools (debugging, visualisation, etc.)</li>
        <li>Systems for model-free and model-based Reinforcement Learning
        </li>
        <li>Optimisation in end-to-end deep learning</li>
        <li> System optimisation using Bayesian Optimisation</li>
        <li> Acceleration of model building (e.g.,
            imitation learning in RL)</li>
        <li> Use of probabilistic models in ML/AI
            application</li>
        <li> Learning models for inferring
            network attacks,
            device/service fingerprinting,
            congestion, etc.</li>
        <li> Techniques to collect and
            analyze network data in a
            privacy-preserving manner</li>
        <li> Learning models to capture
            network events and
            control actions</li>
        <li>Machine learning in
            networking (e.g., use of
            Deep RL in networking)</li>
        <li>Analysis of distributed ML algorithms</li>
        <li>Semantics for distributed ML languages</li>
        <li>Probabilistic modelling for distributed ML algorithms</li>
        <li>Synchronisation and state control of distributed ML algorithms</li>
    </ul>
    <p></p>
<p>Accepted papers will be published in the ACM Digital Library (you can opt out from this).</p>

          </section>
        
          <section class="accepted-papers section" id="accepted-papers">
            
<h4>Accepted Papers</h4>
<h5>Oral Presentation</h5>
<p>
<ul class="list">
  <li>
    <b>"Efficient Multi-Class Classification with Duet"</b> —
    <i>Yiren Zhao, Duo Wang, Daniel Bates, Robert Mullins, Mateja Jamnik, and Pietro Lio (The University of Cambridge)</i>
  </li>
  <br>
  <li>
    <b>"Deep learning on microcontrollers: a study on deployment costs and challenges"</b> —
    <i>Filip Svoboda (University of Cambridge); JAVIER FERNANDEZ-MARQUES (University of Oxford); EDGAR LIBERIS, NICHOLAS LANE (University of Cambridge)</i>
  </li>
  <br>
  <li>
    <b>"yslrn: Learning What to Monitor for Efficient Anomaly Detection"</b> —
    <i>Authors: Davide Sanvito, Giuseppe Siracusano, Sharan Santhanam, Roberto Gonzalez, Roberto Bifulco (NEC Laboratories Europe)</i>
  </li>
  <br>
  <li>
    <b>"BoGraph: Structured Bayesian Optimization From Logs for Expensive Systems with Many"</b> —
    <i>Sami Alabed, Eiko Yoneki (University of Cambridge)</i>
  </li>
  <br>
  <li>
    <b>"Reinforcement Learning for Resource Management in Multi-tenant Serverless Platforms"</b> —
    <i>Haoran Qiu, Weichao Mao, Archit Patke (University of Illinois at Urbana-Champaign); Chen Wang, Hubertus Franke (IBM Thomas J. Watson Research Center); Zbigniew Kalbarczyk, Tamer Başar, Ravishankar K. Iyer (University of Illinois at Urbana-Champaign)</i>
  </li>
  <br>
  <li>
    <b>"Rapid Model Architecture Adaption for Meta-Learning"</b> —
    <i>Yiren Zhao (University of Cambridge); Xitong Gao (Shenzhen Institutes of Advanced Technology); Ilia Shumailov (University of Cambridge); Nicolo Fusi (Microsoft); Robert Mullins (University of Cambridge)</i>
  </li>
  <br>
  <li>
    <b>"How Reinforcement Learning Systems Fail and What to do About It"</b> —
    <i>Pouya Hamdanian (MIT); Malte Schwarzkopf (Brown University); Siddhartha Sen (Microsoft Research); Mohammad Alizadeh (MIT CSAIL)</i>
  </li>
  <br>
  <li>
    <b>"On the Impact of Device and Behavioral Heterogeneity in Federated Learning"</b> —
    <i>Ahmed M. Abdelmoniem (Queen Mary University of London); Chen-Yu Ho, Pantelis Papageorgiou, Marco Canini (KAUST)</i>
  </li>
  <br>
  <li>
    <b>"slo-nns: Service Level Objective-Aware Neural Networks"</b> —
    <i> Daniel Mendoza, Caroline Trippel (Stanford University)</i>
  </li>
  <br>
  <li>
    <b>"FlexHTTP: An Intelligent and Scalable HTTP Version Selection System"</b> —
    <i>Mengying Zhou, Zheng Li, Shihan Lin, Xin Wang, Yang Chen (Fudan University)</i>
  </li>
  <br>
  <li>
    <b>"Live Video Analytics as a Service"</b> —
    <i>Guilherme Henrique Apostolo, Pablo Bauszat, Vinod Nigade, Henri E. Bal, Lin Wang (Vrije Universiteit Amsterdam)</i>
  </li>
  <br>
  <p></p>
  <h5>Poster Presentation</h5>
  <li>
    <b>"dSyncPS: Delayed Synchronization for Dynamic Deployment of Distributed Machine Learning"</b> —
    <i></i>Yibo Guo, An Wang (Case Western Reserve University)</i>
  </li>
  <br>
  <li>
    <b>"Scaling Knowledge Graph Embedding Models"</b> —
    <i>Nasrullah Sheikh, Xiao Qin, Berthold Reinwald (IBM Research Almaden); Chuan Lei (Instacart)</i>
  </li>
  <br>
  <li>
    <b>"Data Selection for Efficient Model Update in Federated Learning"</b> —
    <i>Hongrui Shi, Valentin Radu (University of Sheffield)</i>
  </li>
  <br>
  <li>
    <b>"DyFiP: Explainable AI-based Dynamic Filter Pruning of Convolutional Neural Networks"</b> —
    <i>Muhammad Sabih, Frank Hannig, Jürgen Teich (Friedrich-Alexander-Universität Erlangen-Nürnberg)</i>
  </li>
  <br>
  <li>
    <b>"Apache Submarine: A Unified Machine Learning Platform Made Simple"</b> —
    <i>Kai-Hsun Chen (Academia Sinica); Huan-Ping Su (Union.ai); Wei-Chiu Chuang (Cloudera); Hung-Chang Hsiao (National Cheng Kung University); Wangda Tan (Snowflake); Zhankun Tang (Cloudera); Xun Liu (DiDi); Yanbo Liang (Meta Platforms); Wen-Chih Lo (Chunghwa Telecom); Wanqiang Ji (JD.com); Byron Hsu (UC Berkeley); Keqiu Hu (LinkedIn); HuiYang Jian (KE Holdings); Quan Zhou (Ant Group); Chien-Min Wang (Academia Sinica)</i>
  </li>
  <br>
  <li>
    <b>"Temporal Shift Reinforcement Learning"</b> —
    <i>Deepak George Thomas, Tichakorn Wongpiromsarn, Ali Jannesari (Iowa State University)</i>
  </li>
  <br>
</ul>
</p>

          </section>
        
          <section class="schedule section" id="schedule">
            <h2 class="section-title">Program</h2>
<p>Program timezone is CEST (UTC+2.00).</p>
<div class="schedule-tbl">
  <table>
    <thead>
      <tr>
        <th class="schedule-time"></th>
        <th class="schedule-slot"></th>
      </tr>
    </thead>
    <tbody>
      
        
          <tr class="schedule-other">
            <td class="schedule-time">09:00</td>
            <td class="schedule-slot">Poster Session</td>
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              dSyncPS: Delayed Synchronization for Dynamic Deployment of Distributed Machine
            
              <span class="speakers-company">Yibo Guo, An Wang (Case Western Reserve University)</span>
              <span class="schedule-description program">The increasing demand of applying machine learning technologies in various domains has driven the evolvement of complex machine learning models. To fulfill this demand, distributed machine learning has become the de facto standard computing paradigm for model training. Machine-Learning-as-a-Service (MLaaS) has also emerged as a solution provided by cloud service providers to address this need. With MLaaS, customers can submit their models and training datasets to the service providers, and leverage the existing cloud infrastructure for model training and inference. However, we find that existing solutions are insufficient for end users who requires complex and accurate machine learning models, but with moderate amount of data. The main issue is the lack of support for dynamic deployment of distributed machine learning tasks. To address this issue, we propose a parameter server based framework, called dSyncPS, that allows worker nodes to participate in training dynamically. The key idea is that it separates parameter synchronization from aggregation function in the parameter server nodes, thus resulting in a delayed synchrony.</span>
            </td>
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              Scaling Knowledge Graph Embedding Models
            
              <span class="speakers-company">Nasrullah Sheikh, Xiao Qin, Berthold Reinwald (IBM Research Almaden); Chuan Lei (Instacart)</span>
              <span class="schedule-description program">Developing scalable solutions for training Graph Neural Networks (GNNs) for link prediction tasks is challenging due to the high data dependencies which entail high computational cost and huge memory footprint. We propose a new method for scaling training of knowledge graph embedding models for link prediction to address these challenges. Towards this end, we propose the following algorithmic strategies: self-sufficient partitions, constraint-based negative sampling, and edge mini-batch training. Both, partitioning strategy and constraint-based negative sampling, avoid cross partition data transfer during training. In our experimental evaluation, we show that our scaling solution for GNN-based knowledge graph embedding models achieves a 16x speed up on benchmark datasets while maintaining a comparable model performance as non-distributed methods on standard metrics.</span>
            </td>
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              Data Selection for Efficient Model Update in Federated Learning
            
              <span class="speakers-company">Hongrui Shi, Valentin Radu (University of Sheffield)</span>
              <span class="schedule-description program">The Federated Learning workflow of training a centralized model with distributed data is growing in popularity. However, until recently, this was the realm of contributing clients with similar computing capability. The fast expanding IoT space and data being generated and processed at the edge are encouraging more effort into expanding federated learning to include heterogeneous systems. Previous approaches distribute light models to clients to distill the characteristic of local data into metadata for a partitioned global updates. However, enabling a large size of metadata to transmit in the network will compromise the communication efficiency of FL. We propose to reduce the size of metadata needed for the global update by clustering the activation maps and selecting only the most representative samples. The partitioned global update adopted in our work splits the global CNN model into a lower part for generic feature extraction and an upper part that is more sensitive to the metadata. Our experiments show that only 1.6% of the metadata can effectively transfer the characteristics of the client data to the global model in our slit network approach. These preliminary results evolve our understanding of federated learning by demonstrating efficient training capability with strategic selected training samples.</span>
            </td>
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              DyFiP: Explainable AI-based Dynamic Filter Pruning of Convolutional Neural Networks
            
              <span class="speakers-company">Muhammad Sabih, Frank Hannig, Jürgen Teich (Friedrich-Alexander-Universität Erlangen-Nürnberg)</span>
              <span class="schedule-description program">Filter pruning is one of the most effective ways to accelerate CNN. Most of the existing works are focused on the static pruning of CNN filters. In dynamic pruning of CNN filters, existing works are based on the idea of switching between different branches of a CNN or exiting early based on the harndess of a sample. These approaches can reduce the average latency of inference, but they cannot reduce the longest-path latency of inference. In contrast, we present a novel approach of dynamic filter pruning that utilizes explainable AI along with early coarse prediction in the intermediate layers of a CNN. This coarse prediction is performed using a simple branch that is trained to perform top-k classification. The branch either predicts the output class with high confidence, in which case the rest of the computations are left out. Alternatively, the branch predicts the output class to be within a subset of possible output classes. After this coarse prediction, only those filters that are important for this subset of classes are then evaluated. The importances of filters for each output class are obtained using explainable AI. Using this concept of dynamic pruning, we are able not only to reduce the average latency of inference, but also the longest-path latency of inference. Our proposed architecture for dynamic pruning can be deployed on different hardware platforms.</span>
            </td>
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              Apache Submarine: A Unified Machine Learning Platform Made Simple
            
              <span class="speakers-company">Kai-Hsun Chen (Academia Sinica); Huan-Ping Su (Union.ai); Wei-Chiu Chuang (Cloudera); Hung-Chang Hsiao (National Cheng Kung University); Wangda Tan (Snowflake); Zhankun Tang (Cloudera); Xun Liu (DiDi); Yanbo Liang (Meta Platforms); Wen-Chih Lo (Chunghwa Telecom); Wanqiang Ji (JD.com); Byron Hsu (UC Berkeley); Keqiu Hu (LinkedIn); HuiYang Jian (KE Holdings); Quan Zhou (Ant Group); Chien-Min Wang (Academia Sinica)</span>
              <span class="schedule-description program">As machine learning is applied more widely, it is necessary to have a machine learning platform for both infrastructure administrators and users including expert data scientists and citizen data scientists to improve their productivity. However, existing machine learning platforms are ill-equipped to address the “Machine Learning tech debts” such as glue code, reproducibility, and portability. Furthermore, existing platforms only take expert data scientists into consideration, and thus they are inflexible for infrastructure administrators and non-user-friendly for citizen data scientists. We propose Submarine, a unified machine learning platform, and take all infrastructure administrators, expert data scientists, and citizen data scientists into consideration. Submarine has been widely used in many technology companies, including Ke.com and LinkedIn.</span>
            </td>
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              Temporal Shift Reinforcement Learning
            
              <span class="speakers-company">Deepak George Thomas, Tichakorn Wongpiromsarn, Ali Jannesari (Iowa State University)</span>
              <span class="schedule-description program">The function approximators employed by traditional image-based Deep Reinforcement Learning (DRL) algorithms usually lack a temporal learning component and instead focus on learning the spatial component. We propose a technique, Temporal Shift Reinforcement Learning (TSRL), wherein both temporal, as well as spatial components are jointly learned. Moreover, TSRL does not require additional parameters to perform temporal learning. We show that TSRL outperforms the commonly used frame stacking heuristic on all of the Atari environments we test on while beating the SOTA for all except one of them. This investigation has implications in the robotics as well as sequential decision-making domains.</span>
            </td>
          </tr>
        
      
        
          <tr class="schedule-other">
            <td class="schedule-time">10:00</td>
            <td class="schedule-slot">Coffee Break</td>
          </tr>
        
      
        
          <tr class="schedule-other">
            <td class="schedule-time">10:30</td>
            <td class="schedule-slot">Introduction</td>
          </tr>
        
      
        
          <tr class="schedule-session">
            <td class="schedule-time">15:40</td>
            <td class="schedule-slot">Session 1: Optimisation</td>
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              Efficient Multi-Class Classification with Duet
            
              <span class="speakers-company">Shay Vargaftik, Yaniv Ben-Itzhak (VMware Research)</span>
              <span class="schedule-description program">Accordingly, we propose a new classifier termed Duet. Duet incorporates the advantages of bagging and boosting decision-tree-based ensemble methods (DTEMs) by using two classifiers instead of a monolithic one. A simple bagging model is trained using the entire training dataset and is responsible for capturing the easier concepts. Then, a boosting model is trained using only a fraction of the dataset representing the concepts the bagging model finds hard. To make the whole process resource efficient, we develop a new heuristic approach to rank data with respect to concepts that the bagging model finds hard. We use this approach, termed data instance predictability to determine the dataset fraction for the boosting model training. We implement Duet as a scikit-learn classifier. Evaluation using datasets from different domains and with different characteristics indicates that Duet offers a better tradeoff between classification accuracy and system performance than monolithic DTEMs. Moreover, in an evaluation over a resource-constrained Raspberry Pi 3 device Duet successfully completes all training tasks, where some monolithic models fail due to insufficient resources, indicating broader applicability of Duet to resource-constrained edge devices. Duet is a part of an effort for advancements in resource-efficient classification, and its scikit-learn implementation can be found in https://research.vmware.com/projects/efficient-machine-learning-classification. </span>
            </td>
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              Deep learning on microcontrollers: a study on deployment costs and challenges
            
              <span class="speakers-company">Filip Svoboda (University of Cambridge); JAVIER FERNANDEZ-MARQUES (University of Oxford); EDGAR LIBERIS, NICHOLAS LANE (University of Cambridge)</span>
              <span class="schedule-description program">Deep learning on resource-constrained hardware has become more viable in recent years due to the development of lightweight architectures and compression techniques. Mobile devices are a particularly popular target platform for which major deep learning frameworks offer a streamlined model deployment pipeline. Still, it is possible to run deep neural networks (DNNs) in an even more constrained environment, namely on microcontrollers (MCUs). Microcontrollers are an attractive deployment target due to their low cost, modest power usage and abundance in the wild. However, deploying models to such hardware is non-trivial due to a small amount of on-chip RAM (often < 512KB) and limited compute capabilities. In this work, we delve into the requirements and challenges of fast DNN inference on MCUs: we describe how the memory hierarchy influences the architecture of the model, expose often under-reported costs of compression and quantization techniques, and highlight issues that become critical when deploying to MCUs compared to mobiles. Our findings and experiences are also distilled into a set of guidelines that should ease the future deployment of DNN-based applications on microcontrollers.</span>
            </td>
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              syslrn: Learning What to Monitor for Efficient Anomaly Detection
            
              <span class="speakers-company">Davide Sanvito, Giuseppe Siracusano, Sharan Santhanam, Roberto Gonzalez, Roberto Bifulco (NEC Laboratories Europe)</span>
              <span class="schedule-description program">While monitoring system behavior to detect anomalies and failures is important, existing methods based on log-analysis can only be as good as the information contained in the logs, and other approaches that look at the OS-level software state introduce high overheads. We tackle the problem with syslrn, a system that first builds an understanding of a target system offline, and then tailors the online monitoring instrumentation based on the learned identifiers of normal behavior. While our syslrn prototype is still preliminary and lacks many features, we show in a case study for the monitoring of OpenStack failures that it can outperform state-of-the-art log-analysis systems with little overhead.</span>
            </td>
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
               BoGraph: Structured Bayesian Optimization From Logs for Expensive Systems with Many
            
              <span class="speakers-company">Sami Alabed, Eiko Yoneki (University of Cambridge)</span>
              <span class="schedule-description program">Current auto-tuners struggle with computer systems due to their large complex parameter space and high evaluation cost. We propose BoGraph, an auto-tuning framework that builds a graph of the system components before optimizing it using causal structure learning. The graph contextualizes the system via decomposition of the parameter space for faster convergence and handling of many parameters. Furthermore, BoGraph exposes an API to encode experts' knowledge of the system via performance models and a known dependency structure of the components. We evaluated BoGraph via a hardware design case study achieving 5x-7x5x−7x improvement in energy and latency over the default in a variety of tasks.</span>
            </td>
          </tr>
        
      
        
          <tr class="schedule-other">
            <td class="schedule-time">12:30</td>
            <td class="schedule-slot">Lunch Break / Poster Session </td>
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time">13:45</td>
            <td class="schedule-slot">
            
            
              Keynote 1: Tianqi Chen
            
              <span class="speakers-company">CMU</span>
              <span class="schedule-description program"></span>
            </td>
          </tr>
        
      
        
          <tr class="schedule-session">
            <td class="schedule-time">14:30</td>
            <td class="schedule-slot">Session 2: Reinforcement Learning, Meta-Learning and Federated Learning</td>
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              Reinforcement Learning for Resource Management in Multi-tenant Serverless Platforms
            
              <span class="speakers-company">Haoran Qiu, Weichao Mao, Archit Patke (University of Illinois at Urbana-Champaign); Chen Wang, Hubertus Franke (IBM Thomas J. Watson Research Center); Zbigniew Kalbarczyk, Tamer Başar, Ravishankar K. Iyer (University of Illinois at Urbana-Champaign)</span>
              <span class="schedule-description program">Serverless Function-as-a-Service (FaaS) is an emerging cloud computing paradigm that frees application developers from infrastructure management tasks such as resource provisioning and scaling. To reduce the tail latency of functions and improve resource utilization, recent research has been focused on applying online learning algorithms such as reinforcement learning (RL) to manage resources. Compared to existing heuristics-based resource management approaches, RL-based approaches eliminate humans in the loop and avoid the painstaking generation of heuristics. In this paper, we show that the state-of-the-art single-agent RL algorithm (S-RL) suffers up to 4.6x higher function tail latency degradation on multi-tenant serverless FaaS platforms and is unable to converge during training. We then propose and implement a customized multi-agent RL algorithm based on Proximal Policy Optimization, i.e., multi-agent PPO (MA-PPO). We show that in multi-tenant environments, MA-PPO enables each agent to be trained until convergence and provides online performance comparable to S-RL in single-tenant cases with less than 10% degradation. Besides, MA-PPO provides a 4.4x improvement in S-RL performance (in terms of function tail latency) in multi-tenant cases.
</span>
            </td>
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              Rapid Model Architecture Adaption for Meta-Learning
            
              <span class="speakers-company">Yiren Zhao (University of Cambridge); Xitong Gao (Shenzhen Institutes of Advanced Technology); Ilia Shumailov (University of Cambridge); Nicolo Fusi (Microsoft); Robert Mullins (University of Cambridge)</span>
              <span class="schedule-description program">Network Architecture Search (NAS) methods have recently gathered much attention. They design networks with better performance and use a much shorter search time compared to traditional manual tuning. Despite their efficiency in model deployments, most NAS algorithms target a single task on a fixed hardware system. However, real-life few-shot learning environments often cover a great number of tasks (TT) and deployments on a wide variety of hardware platforms (HH). The combinatorial search complexity T 	imes HT×H creates a fundamental search efficiency challenge if one naively applies existing NAS methods to these scenarios. To overcome this issue, we show, for the first time, how to rapidly adapt model architectures to new tasks in a many-task many-hardware few-shot learning setup by integrating Model Agnostic Meta Learning (MAML) into the NAS flow.</span>
            </td>
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
               How Reinforcement Learning Systems Fail and What to do About It
            
              <span class="speakers-company"> Pouya Hamdanian (MIT); Malte Schwarzkopf (Brown University); Siddhartha Sen (Microsoft Research); Mohammad Alizadeh (MIT CSAIL)</span>
              <span class="schedule-description program">Recent research has turned to Reinforcement Learning (RL) to solve challenging decision problems, as an alternative to hand-tuned heuristics. RL can learn good policies without the need for modeling the environment's dynamics. Despite this promise, RL remains an impractical solution for many real-world systems problems. A particularly challenging case occurs when the environment changes over time, i.e. it exhibits non-stationarity. In this work, we characterize the challenges introduced by non-stationarity and develop a framework for addressing them to train RL agents in live systems. Such agents must explore and learn new environments, without hurting the system's performance, and remember them over time. To this end, our framework (1) identifies different environments encountered by the live system, (2) explores and trains a separate expert policy for each environment, and (3) employs safeguards to protect the system's performance. We apply our framework to straggler mitigation, and evaluate it against a variety of alternative approaches using real-world. We show that each component of our framework is necessary to cope with non-stationarity.</span>
            </td>
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              On the Impact of Device and Behavioral Heterogeneity in Federated Learning
            
              <span class="speakers-company">Ahmed M. Abdelmoniem (Queen Mary University of London); Chen-Yu Ho, Pantelis Papageorgiou, Marco Canini (KAUST)</span>
              <span class="schedule-description program">Federated learning (FL) is becoming a popular paradigm for collaborative learning over distributed, private datasets owned by non-trusting entities. FL has seen successful deployment in production environments, and it has been adopted in services such as virtual keyboards, auto-completion, item recommendation, and several IoT applications. However, FL comes with the challenge of performing training over largely heterogeneous datasets, devices, and networks that are out of the control of the centralized FL server. Motivated by this inherent setting, we make a first step towards characterizing the impact of device and behavioral heterogeneity on the trained model. We conduct an extensive empirical study spanning close to 1.5K unique configurations on five popular FL benchmarks. Our analysis shows that these sources of heterogeneity have a major impact on both model performance and fairness, thus shedding light on the importance of considering heterogeneity in FL system design.</span>
            </td>
          </tr>
        
      
        
          <tr class="schedule-other">
            <td class="schedule-time">15:50</td>
            <td class="schedule-slot">Coffee Break</td>
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time">16:15</td>
            <td class="schedule-slot">
            
            
              Keynote 2: Azalia Mirhoseini
            
              <span class="speakers-company">Google Brain</span>
              <span class="schedule-description program"></span>
            </td>
          </tr>
        
      
        
          <tr class="schedule-session">
            <td class="schedule-time">17:00</td>
            <td class="schedule-slot">Session 3: Applications</td>
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              slo-nns: Service Level Objective-Aware Neural Networks
            
              <span class="speakers-company"> Daniel Mendoza, Caroline Trippel (Stanford University)</span>
              <span class="schedule-description program">Machine learning (ML) inference is a real-time workload that must comply with strict Service Level Objectives (SLOs), including latency and accuracy targets. Unfortunately, ensuring that SLOs are not violated in inference-serving systems is challenging due to inherent model accuracy-latency tradeoffs, SLO diversity across and within application domains, evolution of SLOs over time, unpredictable query patterns, and co-location interference. In this paper, we observe that neural networks exhibit high degrees of per-input activation sparsity during inference. Thus, we propose SLO-Aware Neural Networks which dynamically drop out nodes per-inference query, thereby tuning the amount of computation performed, according to specified SLO optimization targets and machine utilization. slo-nns achieve average speedups of 1.3-56.7	imes1.3−56.7× with little to no accuracy loss (less than 0.3%). When accuracy constrained, slo-nns are able to serve a range of accuracy targets at low latency with the same trained model. When latency constrained, slo-nns can proactively alleviate latency degradation from co-location interference while maintaining high accuracy to meet latency constraints.</span>
            </td>
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              FlexHTTP: An Intelligent and Scalable HTTP Version Selection System
            
              <span class="speakers-company">Mengying Zhou, Zheng Li, Shihan Lin, Xin Wang, Yang Chen (Fudan University)</span>
              <span class="schedule-description program">HTTP has been the primary protocol for web data transmission for decades. Since the late 1990s, HTTP/1.1 has been widely used. Recently, both HTTP/2 and HTTP/3 have been proposed to achieve a better experience on web browsing. However, it is still unknown which of them performs better. In this paper, we leverage the controllable experimental environment of Emulab testbed to conduct a series of measurement studies and find that under different network conditions and web page structures, neither HTTP/2 nor HTTP/3 can always perform better. Motivated by this finding, we propose FlexHTTP, an intelligent and scalable HTTP version selection system. FlexHTTP embeds a supervised machine learning-based classifier to select the appropriate HTTP version according to network conditions and page structures. FlexHTTP adopts a set of distributed agent servers to ensure scalability and keep the classifier up-to-date with the dynamic network. We implement and deploy a proof-of-concept prototype of FlexHTTP on the Emulab testbed. Experiments show that the FlexHTTP achieves a reduction of Speed Index by up to 600ms.</span>
            </td>
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              Live Video Analytics as a Service
            
              <span class="speakers-company">Guilherme Henrique Apostolo, Pablo Bauszat, Vinod Nigade, Henri E. Bal, Lin Wang (Vrije Universiteit Amsterdam)</span>
              <span class="schedule-description program">Many private and public organizations deploy large numbers of cameras, which are used in application services for public safety, healthcare, and traffic control. Recent advances in deep learning have demonstrated remarkable accuracy on computer analytics tasks that are fundamental for these applications, such as object detection and action recognition. While deep learning opens the door for the automation of camera-based applications, deploying pipelines for live video analytics is still a complicated process that requires domain expertise in the fields of machine learning, computer vision, computer systems, and networks. The problem is further amplified when multiple pipelines need to be deployed on the same infrastructure to meet different users' diverse and yet dynamic needs. In this paper, we present a live-video-analytics-as-a-service vision, aiming to remove the complexity barrier and achieve flexibility, agility, and efficiency for applications based on live video analytics. We motivate our vision by identifying its requirements and the shortcomings of existing approaches. Based on our analysis, we present our envisioned system design and discuss the challenges that need to be addressed to make it a reality.</span>
            </td>
          </tr>
        
      
        
          <tr class="schedule-other">
            <td class="schedule-time">18:00</td>
            <td class="schedule-slot">Wrapup</td>
          </tr>
        
      
    </tbody>
  </table>
</div>

          </section>
        
          <section class="sponsors section" id="sponsors">
            <h2 class="section-title">Sponsors</h2>
<br>
<ul class="sponsors-list">

  <li class="sponsor-item" itemscope itemtype="http://schema.org/Organization">
    <a href="https://research.facebook.com/" class="sponsor--link" itemprop="url" target="_blank">
      <img src="img/meta_logo.png" alt="Meta Research" class="photo" itemprop="image">
    </a>
  </li>

  <li class="sponsor-item" itemscope itemtype="http://schema.org/Organization">
    <a href="https://www.acm.org/" class="sponsor--link" itemprop="url" target="_blank">
      <img src="img/acm_logo.png" alt="ACM" class="photo" itemprop="image">
    </a>
  </li>

</ul>


          </section>
        
          <section class="committees section" id="committees">
            <h2 class="section-title">Committees</h2>

<h4>Workshop and TPC Chairs</h4>
<p>
<ul class="list">
    <li>Eiko Yoneki, University of Cambridge, <a href="https://www.cl.cam.ac.uk/~ey204/">https://www.cl.cam.ac.uk/~ey204/</a></li>
    <li>Luigi Nardi, Lund University/Stanford University, <a href="http://cs.lth.se/luigi-nardi/">http://cs.lth.se/luigi-nardi/</a></li>
</ul>
</p>
<p>
<h4>Technical Program Committee</h4>
<p>
    <ul class="list">
        <li>Aaron Zhao, University of Cambridge</li>
        <li>Ahmed M. Abdelmoniem, Queen Mary University of London</li>
        <li>Alexandros Koliousis, NCH</li>
        <li>Amir Payberah, KTH</li>
        <li>Amitabha Roy, Google</li>
        <li>Brooks Paige, UCL</li>
        <li>Chris Cummins, Facebook AI</li>
        <li>Daniel Goodman, Oracle </li>
        <li>Dawei Li, Amazon</li>
        <li>Dimitris Chatzopoulos, University College Dublin</li>
        <li>Fiodar Kazhamiaka,Stanford University</li>
        <li>Guoliang He, University of Cambridge</li>
        <li>Guy Leroy, MSR Cambridge</li>
        <li>Haitham Ammar, Huawei</li>
        <li>Hamed Haddadi, Imperial College London</li>
        <li>Holger Pirk, Imperial College London&lt;</li>
        <li>Jenny Huang, NVIDIA</li>
        <li>Jon Crowcroft, University of Cambridge</li>
        <li>Jose Cano, University of Glasgow</li>
        <li>Keshav Santhanam, Stanford University</li>
        <li>Laurent Bindschaedler, MIT</li>
        <li>Massimiliano Patacchiola, University of Cambridge</li>
        <li>Nikolas Ioannou, IBM Research - Zurich</li>
        <li>Paul Kelly, Imperial College London</li>
        <li>Paul Patras, University of Edinburgh</li>
        <li>Peter Pietzuch, Imperial College London</li>
        <li>Peter Triantafillou, University of Warwick</li>
        <li>Qian Li, Stanford University</li>
        <li>Sam Ainsworth, University of Edinburgh</li>
        <li>Sami Alabed, University of Cambridge</li>
        <li>Stratis Ioannidis, Northeastern University</li>
        <li>Thaleia Dimitra Doudali, IMDEA</li>
        <li>Valentin Radu, University of Sheffield</li>
        <li>Veljko Pejovic, University of Ljubljana</li>
        <li>Zheng Wang, University of Leeds</li>
        <li>Zhihao Jia, CMU</li>
    </ul>
</p>
</p>

<h4>Web Chair</h4>
<ul class="list">
    <li>Alexis Duque, Net AI</li>
</ul>

          </section>
        
          <section class="contact section" id="contact">
            <h2 class="section-title">Contact</h2>
<div>
  <p> For any question(s) related to EuroMLSys 2022, please contact the TPC Chairs <a href="mailto:eiko.yoneki@cl.cam.ac.uk">Eiko Yoneki</a> and <a href="mailto:Luigi.Nardi@cs.lth.se">Luigi Nardi</a>.
  <p><img src="/img/twitter.png" height="50px">Follow us on Twitter: <a href="https://twitter.com/euromlsys">@euromlsys</a>

</div>
          </section>
        

        <footer class="footer">
          <p>Sponsored by
            <a href="https://research.facebook.com/" class="sponsor" itemprop="url" target="_blank">
              <img src="img/meta_logo.png" height="60px" alt="Meta Research" class="photo" itemprop="image">
            </a>
            <a href="https://www.acm.org/" class="sponsor" itemprop="url" target="_blank">
              <img src="img/acm_logo.png" height="60px" alt="ACM" class="photo" itemprop="image">
            </a>
          </p>
          <p>Made with ♥ by EuroMLSys'22 team :).</p>
        </footer>
      </div>
    </div>
  </div>-->ent.write('<script src="/js/jquery.js"><\/script>')</script>
  
  
</body>
</html>
