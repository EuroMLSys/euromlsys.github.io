<!doctype html>
<html itemscope itemtype="http://schema.org/Event">
<head>
	<meta name="generator" content="Hugo 0.54.0" />
  <title itemprop="name">The 3rd Workshop on Machine Learning and Systems (EuroMLSys)</title>

  <meta charset="utf-8">
  <meta name="author" content="The 3rd Workshop on Machine Learning and Systems (EuroMLSys)" />
  <meta name="description" content="The 3rd Workshop on Machine Learning and Systems (EuroMLSys)">
  <meta name="viewport" content="width=device-width">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <meta property="og:title" content="The 3rd Workshop on Machine Learning and Systems (EuroMLSys)" />
<meta property="og:description" content="The 3rd Workshop on Machine Learning and Systems (EuroMLSys)" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://euromlsys.github.io/" />

<meta property="og:image" content="https://euromlsys.github.io/img/euromlsys.png" />


  <meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://euromlsys.github.io/img/euromlsys.png"/>

<meta name="twitter:title" content="The 3rd Workshop on Machine Learning and Systems (EuroMLSys)"/>
<meta name="twitter:description" content="The 3rd Workshop on Machine Learning and Systems (EuroMLSys)"/>


  <link rel="shortcut icon" href="/img/favicon.ico" type="image/x-icon">
  <link rel="apple-touch-icon" href="/img/apple-touch-icon.png">
  <link rel="stylesheet" type="text/css" href="/css/main.css">
  <link rel="stylesheet" type="text/css" href="/css/euromlsys.css">

  <script
  src="https://code.jquery.com/jquery-1.12.4.js"
  integrity="sha256-Qw82+bXyGq6MydymqBxNPYTaUXXq7c8v3CwiYwLLNXU="
  crossorigin="anonymous"></script>
  <script type="text/javascript" src="/js/sections.js"></script>

</head>
<body>
  <div class="global">

    <nav id="nav">
  <ul class="wrapper">
    
      <li class="nav-item">
        <a href="#about" id="link-about" title="Home" class="nav-link">
          Home
        </a>
      </li>
    
      <li class="nav-item">
        <a href="#cfp" id="link-cfp" title="Call for Papers" class="nav-link">
          Call for Papers
        </a>
      </li>
    
      <li class="nav-item">
        <a href="#schedule" id="link-schedule" title="Program" class="nav-link">
          Program
        </a>
      </li>
    
      <li class="nav-item">
        <a href="#submission" id="link-submission" title="Submission" class="nav-link">
          Submission
        </a>
      </li>
    
      <li class="nav-item">
        <a href="#speakers" id="link-speakers" title="Keynote" class="nav-link">
          Keynote
        </a>
      </li>
    
      <li class="nav-item">
        <a href="#sponsors" id="link-sponsors" title="Sponsors" class="nav-link">
          Sponsors
        </a>
      </li>
    
      <li class="nav-item">
        <a href="#committees" id="link-committees" title="Committees" class="nav-link">
          Committees
        </a>
      </li>
    
      <li class="nav-item">
        <a href="#contact" id="link-contact" title="Contact" class="nav-link">
          Contact
        </a>
      </li>
    
  </ul>
</nav>

<hr>

    

<header class="header">
  <div class="wrapper">
    <h1 class="logo-name">
      <a class="logo-link" href="#" title="The 3rd Workshop on Machine Learning and Systems (EuroMLSys)" itemprop="name">The 3rd Workshop on Machine Learning and Systems (EuroMLSys)</a>
    </h1>
    <h2 class="subtitle">co-located with <a href="https://2023.eurosys.org">EuroSys '23</a></h2>
    <h2 class="tagline">May 8th 2023, Rome, Italy</h2>

    <div class="call-action-area">
      

      
    </div>

    <div>
      <img src="/img/euromlsys-white.png">
    </div>
  </div>
</header>

<hr>


    <div class="content" id="content">
      <div class="wrapper">
        
          <section class="about section" id="about">
            <p itemprop="description">
    The recent wave of research focusing on machine intelligence (machine learning and artificial intelligence) and its
    applications has been fuelled by both hardware improvements and deep learning frameworks that simplify the design
    and training of neural models. Advances in AI also accelerate research towards Reinforcement Learning (RL), where
    dynamic control mechanisms are designed to tackle complex tasks. Further, machine learning based optimisation, such
    as Bayesian Optimisation, is gaining traction in the computer systems community where optimisation needs to scale
    with complex and large parameter spaces; areas of interest range from hyperparameter tuning to system configuration
    tuning,
</p>
<p itemprop="description">
    The EuroMLSys workshop will provide a platform for discussing emerging trends in building frameworks, programming
    models, optimisation algorithms, and software engineering tools to support AI/ML applications. At the same time,
    using ML for building such frameworks or optimisation tools will be discussed. EuroMLSys aims to bridge the gap
    between AI research and practice, through a technical program of fresh ideas on software infrastructure, tools,
    design principles, and theory/algorithms (including issues of instability, data efficiency, etc.), from a systems
    perspective. We will also explore potential applications that will take advantage of ML.
</p>


<h4>News</h4>

<p>
  <ul class="list">
      <li><b>The workshop program is up! It will start at 9:00 am.</b></li>
      <li>The workshop venue is <b><DIAD></b> – <a href="http://www.diag.uniroma1.it/en">Dipartimento di Ingegneria Informatica, Automatica e Gestionale Antonio Ruberti</a> (Department of Computer, Control and Management Engineering), Sapienza Università di Roma, Viale Ariosto 25, 00185, Rome, Italy (see <a href="https://2023.eurosys.org/venue.html#reachArea">how to reach the conference area</a>).</li>
      <li>Please join the slack for question/discussion! Anybody can join it. <a href="https://join.slack.com/t/euromlsys/shared_invite/zt-1umzak090-27YelO5ZVvhQJXzkV~z5Zw">https://join.slack.com/t/euromlsys/shared_invite/zt-1umzak090-27YelO5ZVvhQJXzkV~z5Zw</a></li>
      <li>ACM Proceeding will be available on May 8, 2023: <a href="https://dl.acm.org/doi/proceedings/10.1145/3578356">https://dl.acm.org/doi/proceedings/10.1145/3578356</a></li>
  </ul>
  </p>
  </p>


<h4>Key dates</h4>
<p>
<ul class="list">
    <li><b>Paper submission deadline (extended):</b>  March 12, 2023 (23:59 AoE)</li>
    <li><b>Acceptance notification:</b> April 10, 2023</li>
    <li><b>Final paper due:</b>   April 16, 2023</li>
    <li><b>Workshop:</b> May 8, 2023 (full-day workshop)</li>
</ul>
</p>
</p>


<h4>Past Editions</h4>
<p>
<ul class="list">
    <li><a href="https://2021.euromlsys.eu">EuroMLSys 2021</a></li>
    <li><a href="https://2022.euromlsys.eu">EuroMLSys 2022</a></li>
</ul>
</p>
</p>

          </section>
        
          <section class="cfp section" id="cfp">
            <h2 class="section-title">Call for Papers</h2>

<p>EuroMLSys is an interdisciplinary workshop that brings together researchers in computer architecture, systems and machine learning, along with practitioners who are active in these emerging areas.
</p>
<p>Topics of interest include, but are not limited to, the following:</p>
    <ul class="list">
        <li>Scheduling algorithms for data processing clusters</li>
        <li>Custom hardware for machine learning</li>
        <li> Programming languages for machine learning</li>
        <li>Benchmarking systems (for machine learning algorithms)</li>
        <li>Synthetic input data generation for training</li>
        <li> Systems for training and serving machine learning models at scale</li>
        <li>Graph neural networks</li>
        <li>Neural network compression and pruning in systems</li>
        <li> Systems for incremental learning algorithms</li>
        <li> Large scale distributed learning algorithms in practice</li>
        <li>Database systems for large scale learning</li>
        <li>Model understanding tools (debugging, visualisation, etc.)</li>
        <li>Systems for model-free and model-based Reinforcement Learning
        </li>
        <li>Optimisation in end-to-end deep learning</li>
        <li> System optimisation using Bayesian Optimisation</li>
        <li> Acceleration of model building (e.g.,
            imitation learning in RL)</li>
        <li> Use of probabilistic models in ML/AI
            application</li>
        <li> Learning models for inferring
            network attacks,
            device/service fingerprinting,
            congestion, etc.</li>
        <li> Techniques to collect and
            analyze network data in a
            privacy-preserving manner</li>
        <li> Learning models to capture
            network events and
            control actions</li>
        <li>Machine learning in
            networking (e.g., use of
            Deep RL in networking)</li>
        <li>Analysis of distributed ML algorithms</li>
        <li>Semantics for distributed ML languages</li>
        <li>Probabilistic modelling for distributed ML algorithms</li>
        <li>Synchronisation and state control of distributed ML algorithms</li>
    </ul>
    <p></p>
<p>Accepted papers will be published in the ACM Digital Library (you can opt out from this).</p>

          </section>
        
          <section class="schedule section" id="schedule">
            <h2 class="section-title">Program</h2>
<p><b>ACM Proceeding will be available on May 8, 2023 on <a href="https://dl.acm.org/doi/proceedings/10.1145/3578356">ACM Digital Library</a></b></p>
<p>Please join the slack for question/discussion. Anybody can join it. <a href="https://join.slack.com/t/euromlsys/shared_invite/zt-1umzak090-27YelO5ZVvhQJXzkV~z5Zw">Join!</a></p>
<p>Program timezone is CEST (UTC+2.00).</p>
<div class="schedule-tbl">
  <table>
    <thead>
      <tr>
        <th class="schedule-time"></th>
        <th class="schedule-slot"></th>
      </tr>
    </thead>
    <tbody>
      
        
          <tr class="schedule-other">
            <td class="schedule-time">9:00</td>
            <td class="schedule-slot">Opening</td>
            <td></td>
          </tr>
        
      
        
          <tr class="schedule-session">
            <td class="schedule-time">09:15</td>
            <td class="schedule-slot">Session 1: Model, Training and Optimisation - (15mins presentations)</td>
            <td></td>
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              Actionable Data Insights for Machine Learning
            
              <span class="speakers-company">Nils Braun (Apple)</span>
              <span class="schedule-description program">Artificial Intelligence (AI) and Machine Learning (ML) have made tremendous progress in the recent decade and have become ubiquitous in almost all application domains. Many recent advancements in the ease-of-use of ML frameworks and the low-code model training automations have further reduced the threshold for ML model building. As ML algorithms and pre-trained models become commodities, curating the appropriate training datasets and model evaluations remain critical challenges. However, these tasks are labor-intensive and require ML practitioners to have bespoke data skills. Based on the feedback from different ML projects, we
built ADIML (Actionable Data Insights for ML) – a holistic data toolset. The goal is to democratize data-centric ML approaches by removing big data and distributed system barriers for engineers. We show in several case studies how the application of ADIML has helped solve specific data challenges and shorten the time to obtain actionable insights.</span>
            </td>
            
            <td class="schedule-resources">
              <a href="pdf/oral/O12/12_slides.pdf" target="_blank"><img src="img/paper-64.png" height="15" valign="middle"></a>
            </td>
            
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              Dynamic Stashing Quantization for Efficient Transformer Training
            
              <span class="speakers-company">Guo Yang (University of Cambridge)</span>
              <span class="schedule-description program">Large Language Models (LLMs) have demonstrated impressive performance on a range of Natural Language Processing (NLP) tasks. Unfortunately, the immense amount of computations and memory accesses required for LLM training makes them prohibitively expensive in terms of hardware cost, and thus challenging to deploy in use cases such as on-device learning.

In this paper, motivated by the observation that LLM training is memory-bound, we propose a novel dynamic quantization strategy, termed Dynamic Stashing Quantization (DSQ), that puts a special focus on reducing the memory operations, but also enjoys the other benefits of low precision training, such as the reduced arithmetic cost. We conduct a thorough study on two translation tasks (trainedfrom-scratch) and three classification tasks (fine-tuning). DSQ reduces the amount of arithmetic operations by 20.95X and the number of DRAM operations by 2.55X on IWSLT17 compared to the standard 16-bit fixed-point, which is widely used in on-device learning.</span>
            </td>
            
            <td class="schedule-resources">
              <a href="https://youtu.be/tBY6mjJjGK8" target="_blank"><img src="img/video-48.png" height="15" valign="middle"></a><br>
              <a href="pdf/oral/O11/11_slides.pdf" target="_blank"><img src="img/paper-64.png" height="15" valign="middle"></a>
            </td>
            
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              Towards A Platform for Model Training on Dynamic Datasets 
            
              <span class="speakers-company">Maximilian Böther (ETHZ)</span>
              <span class="schedule-description program">Machine learning (ML) is often applied in use cases where training data evolves and/or grows over time. Training must incorporate data changes for high model quality, however this is often challenging and expensive due to large datasets and models. In contrast, ML researchers often train and evaluate ML models on static datasets or with artificial assumptions about data dynamics. This gap between research and practice is largely due to (i) the absence of an open-source platform that manages dynamic datasets at scale and supports pluggable policies for when and what data to train on, and (ii) the lack of representative open-source benchmarks for ML training on dynamic datasets. To address this gap, we propose to design a platform that enables ML researchers and practitioners to explore training and data selection policies, while alleviating the burdens of managing large dynamic datasets and orchestrating recurring training jobs. We also propose to build an accompanying benchmark suite that integrates public dynamic datasets and ML models from a variety of representative use cases.</span>
            </td>
            
            <td class="schedule-resources">
              <a href="pdf/oral/O16/16_poster.pdf" target="_blank"><img src="img/paper-64.png" height="15" valign="middle"></a>
            </td>
            
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              Profiling and Monitoring Deep Learning Training Tasks
            
              <span class="speakers-company">Ehsan Yousefzadeh-Asl-Miandoab (IT University of Copenhagen)</span>
              <span class="schedule-description program">The embarrassingly parallel nature of deep learning training tasks makes CPU-GPU co-processors the primary commodity hardware for them. The computing and memory requirements of these tasks, however, do not always align well with the available GPU resources. It is, therefore, important to monitor and profile the behavior of training tasks on co-processors to understand better the requirements of different use cases. In this paper, our goal is to shed more light on the variety of tools for profiling and monitoring deep learning training tasks on server-grade NVIDIA GPUs. In addition to surveying the main characteristics of the tools, we analyze the functional limitations and overheads of each tool by using a both light and heavy training scenario. Our results show that monitoring tools like nvidia-smi and dcgm can be integrated with resource managers for online decision making thanks to their low overheads. On the other hand, one has to be careful about the set of metrics to correctly reason about the GPU utilization. When it comes to profiling, each tool has its time to shine; a framework-based or system-wide GPU profiler can first detect the frequent kernels or bottlenecks, and then, a lower-level GPU profiler can focus on particular kernels at the micro-architectural-level.</span>
            </td>
            
            <td class="schedule-resources">
              <a href="https://www.youtube.com/watch?v=-dZkowi_zpM" target="_blank"><img src="img/video-48.png" height="15" valign="middle"></a><br>
              <a href="pdf/oral/O20/20_poster.pdf" target="_blank"><img src="img/paper-64.png" height="15" valign="middle"></a>
            </td>
            
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              MCTS-GEB: Monte Carlo Tree Search is a Good E-graph Builder 
            
              <span class="speakers-company">Guoliang He (University of Cambridge)</span>
              <span class="schedule-description program">Rewrite systems [ 11, 16, 18] have been widely employing equality saturation [15], which is an optimisation methodology that uses a saturated e-graph to represent all possible sequences of rewrite simultaneously, and then extracts the optimal one. As such, optimal results can be achieved by avoiding the phase-ordering problem. However, we observe that when the e-graph is not saturated, it cannot represent all possible rewrite opportunities and therefore the phase-ordering problem is re-introduced during the construction phase of the e-graph. To address this problem, we propose MCTS-GEB, a domain-general rewrite system that applies reinforcement learning (RL) to e-graph construction. At its core, MCTS-GEB uses a Monte Carlo Tree Search (MCTS) [4] to efficiently plan for the optimal e-graph construction, and therefore it can effectively eliminate the phase-ordering problem at the construction phase and achieve better performance within a reasonable time. Evaluation in two different domains shows MCTS-GEB can outperform the state-of-the-art rewrite systems by up to 49x, while the optimisation can generally take less than an hour, indicating MCTS-GEB is a promising building block for the future generation of rewrite systems.</span>
            </td>
            
            <td class="schedule-resources">
              <a href="https://youtu.be/wvrcJVsGU5Y" target="_blank"><img src="img/video-48.png" height="15" valign="middle"></a><br>
              <a href="pdf/oral/O8/8_poster.pdf" target="_blank"><img src="img/paper-64.png" height="15" valign="middle"></a>
            </td>
            
          </tr>
        
      
        
          <tr class="schedule-other">
            <td class="schedule-time">10:00</td>
            <td class="schedule-slot">Coffee Break</td>
            <td></td>
          </tr>
        
      
        
          <tr class="schedule-session">
            <td class="schedule-time">10:30</td>
            <td class="schedule-slot">Session 2: Decentralised Learning, Federated Learning - (15mins presentations)</td>
            <td></td>
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              Decentralized Learning Made Easy with DecentralizePy
            
              <span class="speakers-company">Rishi Sharma (EPFL)</span>
              <span class="schedule-description program">Decentralized learning (DL) has gained prominence for its potential benefits in terms of scalability, privacy, and fault tolerance. It consists of many nodes that coordinate without a central server and exchange millions of parameters in the inherently iterative process of machine learning (ML) training. In addition, these nodes are connected in complex and potentially dynamic topologies. Assessing the intricate dynamics of such networks is clearly not an easy task. Often in literature, researchers resort to simulated environments that do not scale and fail to capture practical and crucial behaviors, including the ones associated to parallelism, data transfer, network delays, and wall-clock time. In this paper, we propose DecentralizePy, a distributed framework for decentralized ML, which allows for the emulation of large-scale learning networks in arbitrary topologies. We demonstrate the capabilities of DecentralizePy by deploying techniques such as sparsification and secure aggregation on top of several topologies, including dynamic networks with more than one thousand nodes.</span>
            </td>
            
            <td class="schedule-resources">
              <a href="https://www.youtube.com/watch?v=OW_-lggBmIw" target="_blank"><img src="img/video-48.png" height="15" valign="middle"></a><br>
              <a href="pdf/oral/O18/18_slides.pdf" target="_blank"><img src="img/paper-64.png" height="15" valign="middle"></a>
            </td>
            
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              Towards Practical Few-shot Federated NLP
            
              <span class="speakers-company">Dongqi Cai (Beiyou Shenzhen Institute)</span>
              <span class="schedule-description program">Transformer-based pre-trained models have emerged as the predominant solution for natural language processing (NLP).
Fine-tuning such pre-trained models for downstream tasks often requires a considerable amount of labeled private data.
In practice, private data is often distributed across heterogeneous mobile devices and may be prohibited from being uploaded.
Moreover, well-curated labeled data is often scarce, presenting an additional challenge.
To address these challenges, we first introduce a data generator for federated few-shot learning tasks, which encompasses the quantity and skewness of scarce labeled data in a realistic setting.
Subsequently, we propose AUG-FedPrompt, a prompt-based fed}erated learning system that exploits abundant unlabeled data for data augmentation.
Our experiments indicate that AUG-FedPrompt can perform on par with full-set fine-tuning with a limited amount of labeled data.
However, such competitive performance comes at a significant system cost.</span>
            </td>
            
            <td class="schedule-resources">
              <a href="https://www.youtube.com/watch?v=7O88PnYi2NU" target="_blank"><img src="img/video-48.png" height="15" valign="middle"></a><br>
              <a href="pdf/oral/O5/5_poster.pdf" target="_blank"><img src="img/paper-64.png" height="15" valign="middle"></a>
            </td>
            
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              Towards Robust and Bias-free Federated Learning
            
              <span class="speakers-company">Ousmane Touat (LIRIS INSA Lyon)</span>
              <span class="schedule-description program">Federated learning (FL) is an exciting machine learning approach where multiple devices collaboratively train a model without sharing their raw data. The FL system is vulnerable to the action of Byzantine clients sending arbitrary model updates, and the trained model may exhibit prediction bias towards specific groups. However, FL mechanisms tackling robustness and bias mitigation have contradicting objectives, motivating the question of building a FL system that comprehensively combines both objectives. 
In this paper, we first survey state-of-the-art approaches to robustness to Byzantine behavior and bias mitigation and analyze their respective objectives. Then, we conduct an empirical evaluation to illustrate the interplay between state-of-the-art FL robustness mechanisms and FL bias mitigation mechanisms. Specifically, we show that classical robust FL methods may inadvertently filter out benign FL clients that have statistically rare data, particularly for minority groups. Finally, we derive research directions for building more robust and bias-free FL systems.</span>
            </td>
            
            <td class="schedule-resources">
              <a href="https://www.youtube.com/watch?v=1d9lWumJHZI" target="_blank"><img src="img/video-48.png" height="15" valign="middle"></a><br>
              <a href="pdf/oral/O6/6_slides.pdf" target="_blank"><img src="img/paper-64.png" height="15" valign="middle"></a>
            </td>
            
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              Gradient-less Federated Gradient Boosting Tree with Learnable Learning Rate
            
              <span class="speakers-company">Chenyang Ma (University of Cambridge)</span>
              <span class="schedule-description program">The privacy-sensitive nature of decentralized datasets and the robustness of eXtreme Gradient Boosting (XGBoost) on tabular data raise the needs to train XGBoost in the context of federated learning (FL). Existing works on federated XGBoost in the horizontal setting rely on the sharing of gradients, which induce per-node level communication frequency and serious privacy concerns. To alleviate these problems, we develop an innovative framework for horizontal federated XGBoost which does not depend on the sharing of gradients and simultaneously boosts privacy and communication efficiency by making the learning rates of the aggregated tree ensembles learnable. We conduct extensive evaluations on various classification and regression datasets, showing our approach achieves performance comparable to the state-of-the-art method and effectively improves communication efficiency by lowering both communication rounds and communication overhead by factors ranging from 25x to 700x.</span>
            </td>
            
            <td class="schedule-resources">
              <a href="https://youtu.be/9qerzP8VbxI" target="_blank"><img src="img/video-48.png" height="15" valign="middle"></a><br>
              <a href="pdf/oral/O10/10_poster.pdf" target="_blank"><img src="img/paper-64.png" height="15" valign="middle"></a>
            </td>
            
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              Distributed Training for Speech Recognition using Local Knowledge Aggregation and Knowledge Distillation in Heterogeneous Systems
            
              <span class="speakers-company">Valentin Radu (U. Sheffield)</span>
              <span class="schedule-description program">Data privacy and protection are crucial issues for any automatic speech recognition (ASR) system when relying on client generated data for training. The best protection is achieved when training is distributed closer to the client local data, rather than centralising the training. However, distributed training suffers from system heterogeneity, due to clients having unequal computation resources, and data heterogeneity, due to training data being non-independent and identically distributed (non-IID). To tackle these challenges, we introduce FedKAD, a Federated Learning (FL) framework that uses local Knowledge Aggregation over top level feature maps and Knowledge Distillation. We show that our FedKAD achieves better communication efficiency than standard FL methods that use uniform models, due to transferring parameters of smaller size client models, and overall better accuracy than FedMD, an alternative KD-based approach designed for heterogeneous data. Our work enables faster, cheaper and more inclusive participation of clients in heterogeneous distributed training.</span>
            </td>
            
            <td class="schedule-resources">
              <a href="https://www.youtube.com/watch?v=1h-NIpMMvH8" target="_blank"><img src="img/video-48.png" height="15" valign="middle"></a><br>
              <a href="pdf/oral/O22/22_slides.pdf" target="_blank"><img src="img/paper-64.png" height="15" valign="middle"></a>
            </td>
            
          </tr>
        
      
        
          <tr class="schedule-other">
            <td class="schedule-time">12:15</td>
            <td class="schedule-slot">Poster Elevator Pitch</td>
            <td></td>
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              Best of both, Structured and Unstructured Sparsity in Neural Networks
            
              <span class="speakers-company">Sven Wagner (Bosch Sicherheitssysteme GmbH)</span>
              <span class="schedule-description program"></span>
            </td>
            
            <td class="schedule-resources">
              <a href="https://www.youtube.com/watch?v=fv_104gDh6U" target="_blank"><img src="img/video-48.png" height="15" valign="middle"></a><br>
              <a href="pdf/poster/P14/14_poster.pdf" target="_blank"><img src="img/paper-64.png" height="15" valign="middle"></a>
            </td>
            
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              TSMix: time series data augmentation by mixing sources
            
              <span class="speakers-company">Artjom Joosen (Huawei)</span>
              <span class="schedule-description program"></span>
            </td>
            
            <td class="schedule-resources">
              <a href="https://www.youtube.com/watch?v=32v9kh3aBSQ" target="_blank"><img src="img/video-48.png" height="15" valign="middle"></a><br>
              <a href="pdf/poster/P15/15_poster.pdf" target="_blank"><img src="img/paper-64.png" height="15" valign="middle"></a>
            </td>
            
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              Toward Pattern-based Model Selection for Cloud Resource Forecasting
            
              <span class="speakers-company">Georgia Christofidi &amp; Konstantinos Papaioannou (IMDEA Software Institute)</span>
              <span class="schedule-description program"></span>
            </td>
            
            <td class="schedule-resources">
              <a href="https://www.youtube.com/watch?v=2pwflH52L54" target="_blank"><img src="img/video-48.png" height="15" valign="middle"></a><br>
              <a href="pdf/poster/P19/19_poster.pdf" target="_blank"><img src="img/paper-64.png" height="15" valign="middle"></a>
            </td>
            
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              Can Fair Federated Learning Reduce the need for Personalisation?
            
              <span class="speakers-company">Alex Iacob (University of Cambridge)</span>
              <span class="schedule-description program"></span>
            </td>
            
            <td class="schedule-resources">
              <a href="https://youtu.be/lPUlXiN0aII" target="_blank"><img src="img/video-48.png" height="15" valign="middle"></a><br>
              <a href="pdf/poster/P23/23_poster.pdf" target="_blank"><img src="img/paper-64.png" height="15" valign="middle"></a>
            </td>
            
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              A First Look at the Impact of Distillation Hyper-Parameters in Federated Knowledge Distillation
            
              <span class="speakers-company">Norah Alballa (KAUST)</span>
              <span class="schedule-description program"></span>
            </td>
            
            <td class="schedule-resources">
              <a href="https://youtu.be/albnlK0Ouow" target="_blank"><img src="img/video-48.png" height="15" valign="middle"></a><br>
              <a href="pdf/poster/P21/21_poster.pdf" target="_blank"><img src="img/paper-64.png" height="15" valign="middle"></a>
            </td>
            
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              Causal fault localisation in dataflow systems
            
              <span class="speakers-company">Andrei Paleyes (University of Cambridge)</span>
              <span class="schedule-description program"></span>
            </td>
            
            <td class="schedule-resources">
              <a href="https://youtu.be/nIS9vqvTFIs" target="_blank"><img src="img/video-48.png" height="15" valign="middle"></a><br>
              <a href="pdf/poster/P29/29_poster.pdf" target="_blank"><img src="img/paper-64.png" height="15" valign="middle"></a>
            </td>
            
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              Accelerating Model Training: Performance Antipatterns Eliminator Framework
            
              <span class="speakers-company">Ravi Singh (TCS Research)</span>
              <span class="schedule-description program"></span>
            </td>
            
            <td class="schedule-resources">
              <a href="https://youtu.be/kF_oasPn_lg" target="_blank"><img src="img/video-48.png" height="15" valign="middle"></a><br>
              <a href="pdf/poster/P33/33_poster.pdf" target="_blank"><img src="img/paper-64.png" height="15" valign="middle"></a>
            </td>
            
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              TinyMLOps for real-time ultra-low power MCUs applied to frame-based event classification
            
              <span class="speakers-company">Minh Tri Lê (Inria Grenoble Rhône-Alpes)</span>
              <span class="schedule-description program"></span>
            </td>
            
            <td class="schedule-resources">
              <a href="https://www.youtube.com/watch?v=W9c4JYb0v90%20" target="_blank"><img src="img/video-48.png" height="15" valign="middle"></a><br>
              <a href="pdf/poster/P17/17_poster.pdf" target="_blank"><img src="img/paper-64.png" height="15" valign="middle"></a>
            </td>
            
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              Scalable High-Performance Architecture for Evolving Recommender System 
            
              <span class="speakers-company">Ravi Singh (TCS Research)</span>
              <span class="schedule-description program"></span>
            </td>
            
            <td class="schedule-resources">
              <a href="https://youtu.be/ZlfLH94ZeZ8%20" target="_blank"><img src="img/video-48.png" height="15" valign="middle"></a><br>
              <a href="pdf/poster/P30/30_poster.pdf" target="_blank"><img src="img/paper-64.png" height="15" valign="middle"></a>
            </td>
            
          </tr>
        
      
        
          <tr class="schedule-other">
            <td class="schedule-time">13:00</td>
            <td class="schedule-slot">Lunch Break / Poster Session</td>
            <td></td>
          </tr>
        
      
        
          <tr class="schedule-session">
            <td class="schedule-time">14:30</td>
            <td class="schedule-slot">Session 3: Service Functions, TinyML, CDN - (15mins presentations)</td>
            <td></td>
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              FoldFormer: sequence folding and seasonal attention for fine-grained long-term FaaS forecasting
            
              <span class="speakers-company">Luke Darlow (Huawei)</span>
              <span class="schedule-description program">Fine-grained long-term (FGLT) time series forecasting is a fundamental challenge in Function as a Service (FaaS) platforms. The data that FaaS function requests produce are fine-grained (per-second/minute), often have daily periodicity, and are persistent over the long term. Forecasting in the FGLT data regime is challenging, and Transformer models can scale poorly for long sequences. We propose FoldFormer that combines several novel elements – time-to-latent folding, seasonal attention, and convolutions over FFT representations – as a new solution for FGLT forecasting of FaaS function requests. FoldFormer is designed to efficiently consume very fine-grained multi-day data with nearly no additional model, memory, or compute overhead, when compared to consuming coarse-grained data. We show either state-of-the-art or competitive performance for per-minute function requests on the top 5 most requested functions for three data sources, including two in-house Huawei Cloud sources and Azure 2019. We also show state-of-the-art performance at per-second granularity — a regime that critically limits most other methods.</span>
            </td>
            
            <td class="schedule-resources">
              <a href="pdf/oral/O13/13_slides.pdf" target="_blank"><img src="img/paper-64.png" height="15" valign="middle"></a>
            </td>
            
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              Reconciling High Accuracy, Cost-Efficiency, and Low Latency of Inference Serving Systems
            
              <span class="speakers-company">Alireza Sanaee (Queen Mary University of London)</span>
              <span class="schedule-description program">The use of machine learning (ML) inference for various applications is growing drastically. ML inference services engage with users directly, requiring fast and accurate responses. Moreover, these services face dynamic workloads of requests, imposing changes in their computing resources. Failing to right-size computing resources results in either latency service level objectives (SLOs) violations or wasted computing resources. Adapting to dynamic workloads considering all the pillars of accuracy, latency, and resource cost is challenging. In response to these challenges, we propose an adaptation mechanism, InfAdapter, that proactively selects a set of ML model variants with their resource allocations to meet latency SLO while maximizing an objective function composed of accuracy and cost. InfAdapter decreases SLO violation and costs up to 65% and 33%, respectively, compared to a popular industry autoscaler (Kubernetes Vertical Pod Autoscaler).</span>
            </td>
            
            <td class="schedule-resources">
              <a href="https://www.youtube.com/watch?v=sVbLuNO-25o" target="_blank"><img src="img/video-48.png" height="15" valign="middle"></a><br>
              <a href="pdf/oral/O9/9_slides.pdf" target="_blank"><img src="img/paper-64.png" height="15" valign="middle"></a>
            </td>
            
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              Robust and Tiny Binary Neural Networks using Gradient-based Explainability Methods
            
              <span class="speakers-company">Muhammad Sabih (Friedrich-Alexander)</span>
              <span class="schedule-description program">Binary neural networks (BNNs) are a highly resource-efficient variant of neural networks. The efficiency of BNNs for tiny machine learning (TinyML) systems can be enhanced by structured pruning and making BNNs robust to faults. This fault tolerance can be traded off for energy consumption, latency, or cost when used with approximate memory systems. For pruning, magnitude-based heuristics are not useful because the weights in a BNN can either be -1 or +1. Global pruning of BNNs has not been studied well so far. Thus, in this paper, we explore gradient-based ranking criteria for pruning BNNs and use them in combination with a sensitivity analysis. 
For robustness, the state-of-the-art is to train the BNNs with bit-flips in what is known as fault-aware training. We propose a method to guide fault-aware training using gradient-based explainability methods. This allows us to obtain robust and efficient BNNs for deployment on tiny devices. Experiments on audio and image processing applications show that our proposed approach outperforms the existing approaches, making it useful for obtaining efficient and robust models for a slight degradation in accuracy. This makes our approach valuable for many TinyML use cases.</span>
            </td>
            
            <td class="schedule-resources">
              <a href="https://youtu.be/KubgKuphZjo" target="_blank"><img src="img/video-48.png" height="15" valign="middle"></a><br>
              <a href="pdf/oral/O31/31_slides.pdf" target="_blank"><img src="img/paper-64.png" height="15" valign="middle"></a>
            </td>
            
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time"></td>
            <td class="schedule-slot">
            
            
              Illuminating the hidden challenges of data-driven CDNs
            
              <span class="speakers-company">Theophilus A. Benson (CMU)</span>
              <span class="schedule-description program">While Data-driven CDNs have the potential to provide un- paralleled performance and availability improvements, they open up an intricate and exciting tapestry of previously un- addressed problems. This paper highlights these problems, explores existing solutions, and identifies open research questions for each direction. We, also, present a strawman approach, Guard-Rails, that embodies preliminary techniques that can be used to help safeguard data-driven CDNs against the identified perils.</span>
            </td>
            
            <td class="schedule-resources">
              <a href="pdf/oral/O4/4_slides.pdf" target="_blank"><img src="img/paper-64.png" height="15" valign="middle"></a>
            </td>
            
          </tr>
        
      
        
          <tr class="schedule-other">
            <td class="schedule-time">15:30</td>
            <td class="schedule-slot">Poster Session</td>
            <td></td>
          </tr>
        
      
        
          <tr class="schedule-other">
            <td class="schedule-time">16:00</td>
            <td class="schedule-slot">Coffee Break</td>
            <td></td>
          </tr>
        
      
        
          <tr class="schedule-talk">
            <td class="schedule-time">16:30</td>
            <td class="schedule-slot">
            
            
              Keynote: Next-Generation Domain-Specific Accelerators: From Hardware to System
            
              <span class="speakers-company">Sophia Shao (UC Berkeley)</span>
              <span class="schedule-description program">Decades of exponential growth in computing have transformed the way our society operates. As the benefits of traditional technology scaling fade, the computing industry has started developing vertically integrated systems with specialized accelerators to deliver improved performance and energy efficiency. In fact, domain-specific accelerators have become a key component in today’s systems-on-chip (SoCs) and systems-on-package (SoPs), driving active research and product development to build novel accelerators for emerging applications such as machine learning, robotics, cryptography, and many more, entering a golden edge for computer architecture. The natural evolution of this trend will lead to an increasing volume and diversity of accelerators on future computing platforms. In this talk, I will discuss challenges and opportunities for the next-generation of domain-specific accelerators, with a special focus on system-level implications of designing, integrating, and scheduling of future heterogeneous platforms.</span>
            </td>
            
            <td class="schedule-resources">
              <a href="pdf/shao-euromlsys-05-2023.pdf" target="_blank"><img src="img/paper-64.png" height="15" valign="middle"></a>
            </td>
            
          </tr>
        
      
        
          <tr class="schedule-other">
            <td class="schedule-time">18:00</td>
            <td class="schedule-slot">Wrapup and Closing</td>
            <td></td>
          </tr>
        
      
    </tbody>
  </table>
</div>

          </section>
        
          <section class="submission section" id="submission">
            <h2 class="section-title">Submission</h2>

<p>Papers must be submitted electronically as PDF files, formatted for 8.5x11-inch paper. The length of the paper must be no more than 6 pages in the ACM double-column format (10-pt font). References are out of the 6 pages limit. Submitted papers must use the official <a
        href="https://www.acm.org/publications/proceedings-template">ACM Master article template</a></p>
<p> Submissions will be single-blind.</p>

<p><b>Submit your paper at:</b> <a href="https://euromlsys23.hotcrp.com/paper/new">https://euromlsys23.hotcrp.com/paper/new</a></p>

          </section>
        
          <section class="speakers section" id="speakers">
            <h2 class="section-title">Keynote</h2>
<p> </p>
<ul class="speakers-list">

  <li class="speakers-item" itemprop="performer" itemscope itemtype="http://schema.org/Person">
    
      <span class="speaker-photo">
        <img class="photo" src="/img/shao-photo.jpeg" alt="Sophia Shao" itemprop="image">
      </span>
    

    <h3 class="speech-title">
      
      <span class="speech-time">16:30</span>
      
      <span>Sophia Shao</span>
      <span class="speakers-company">University of California</span>
    </h3>

    <h3 class="speakers-name">Next-Generation Domain-Specific Accelerators: From Hardware to System
      
    </h3>
    
      <p class="schedule-resources">
        <a href="pdf/shao-euromlsys-05-2023.pdf" target="_blank">Slides <img src="img/paper-64.png" height="15" valign="middle"></a>
      </p>
      

    <p class="schedule-description">Decades of exponential growth in computing have transformed the way our society operates. As the benefits of traditional technology scaling fade, the computing industry has started developing vertically integrated systems with specialized accelerators to deliver improved performance and energy efficiency. In fact, domain-specific accelerators have become a key component in today’s systems-on-chip (SoCs) and systems-on-package (SoPs), driving active research and product development to build novel accelerators for emerging applications such as machine learning, robotics, cryptography, and many more, entering a golden edge for computer architecture. The natural evolution of this trend will lead to an increasing volume and diversity of accelerators on future computing platforms. In this talk, I will discuss challenges and opportunities for the next-generation of domain-specific accelerators, with a special focus on system-level implications of designing, integrating, and scheduling of future heterogeneous platforms.</p>
    <p class="speakers-bio">Bio:  Professor Sophia Shao is an Assistant Professor of Electrical Engineering and Computer Sciences at the University of California, Berkeley. Previously, she was a Senior Research Scientist at NVIDIA and received her Ph.D. degree in 2016 from Harvard University. Her research interests are in the area of computer architecture, with a special focus on domain-specific architecture, deep-learning accelerators, and high-productivity hardware design methodology. Her work has been awarded the Best Paper Award at DAC’2021, the Best Paper Award at JSSC’2020, a Best Paper Award at MICRO’2019, a Research Highlight of Communications of ACM (2021), Top Picks in Computer Architecture (2014), and Honorable Mentions (2019*2). Her Ph.D. dissertation was nominated by Harvard for the ACM Doctoral Dissertation Award. She is a recipient of an NSF CAREER Award, the 2022 IEEE TCCA Young Computer Architect Award, an Intel Rising Star Faculty Award, a Google Faculty Rising Star Award in System Research, a Facebook Research Award, and the inaugural Dr. Sudhakar Yalamanchili Award. Her personal webpage is https://people.eecs.berkeley.edu/~ysshao/.</p>

  </li>

</ul>

          </section>
        
          <section class="sponsors section" id="sponsors">
            <h2 class="section-title">Sponsors</h2>
<br>
<ul class="sponsors-list">

  <li class="sponsor-item" itemscope itemtype="http://schema.org/Organization">
    <a href="https://research.facebook.com/" class="sponsor--link" itemprop="url" target="_blank">
      <img src="img/meta_logo.png" alt="Meta Research" class="photo" itemprop="image">
    </a>
  </li>

  <li class="sponsor-item" itemscope itemtype="http://schema.org/Organization">
    <a href="https://www.acm.org/" class="sponsor--link" itemprop="url" target="_blank">
      <img src="img/acm_logo.png" alt="ACM" class="photo" itemprop="image">
    </a>
  </li>

  <li class="sponsor-item" itemscope itemtype="http://schema.org/Organization">
    <a href="https://www.tolacapital.com/" class="sponsor--link" itemprop="url" target="_blank">
      <img src="img/tola.png" alt="Tola Capital" class="photo" itemprop="image">
    </a>
  </li>

</ul>


          </section>
        
          <section class="committees section" id="committees">
            <h2 class="section-title">Committees</h2>

<h4>Workshop and TPC Chairs</h4>
<p>
<ul class="list">
    <li>Eiko Yoneki, University of Cambridge, <a href="https://www.cl.cam.ac.uk/~ey204/">https://www.cl.cam.ac.uk/~ey204/</a></li>
    <li>Luigi Nardi, Lund University/Stanford University, <a href="http://cs.lth.se/luigi-nardi/">http://cs.lth.se/luigi-nardi/</a></li>
</ul>
</p>
<p>
<h4>Technical Program Committee</h4>
<p>
    <ul class="list">
        <li>Aaron Zhao, Imperial College London</li>
        <li>Ahmed M. Abdelmoniem, Queen Mary University of London</li>
        <li>Alexandros Koliousis, Northeastern University London and Institute for Experiential AI</li>
        <li>Amir Payberah, KTH</li>
        <li>Amitabha Roy, Kumo.ai</li>
        <li>Chi Zhang, Brandeis University</li>
        
        <li>Daniel Goodman, Oracle </li>
        <li>Daniel Mendoza, Stanford University</li>
        <li>Davide Sanvito, NEC Laboratories Europe</li>
        <li>Dawei Li, Amazon</li>
        <li>Deepak George Thomas, Iowa State University </li>
        <li>Dimitris Chatzopoulos, University College Dublin</li>
        <li>Fiodar Kazhamiaka,Stanford University</li>
        <li>Guilherme H. Apostolo, Vrije Universiteit Amsterdam</li>
        <li>Guoliang He, University of Cambridge</li>
        
        <li>Hamed Haddadi, Imperial College London</li>
        
        <li>Jenny Huang, NVIDIA</li>
        <li>Jon Crowcroft, University of Cambridge</li>
        <li>Jose Cano, University of Glasgow</li>
        <li>Junru Shao, OctoML</li>
        <li>Keshav Santhanam, Stanford University</li>
        <li>Liang Zhang, TigerGraph</li>
        <li>Lianmin Zheng, UC Berkeley</li>
        
        
        
        <li>Mengying Zhou, Fudan University</li>
        <li>Nasrullah Sheikh, IBM Research Almaden</li>
        <li>Nikolas Ioannou, Google</li>
        <li>Paul Patras, University of Edinburgh</li>
        <li>Peter Pietzuch, Imperial College London</li>
        <li>Peter Triantafillou, University of Warwick</li>
        <li>Pouya Hamadanian, MIT</li>
        <li>Pratik Fegade, Google</li>
        <li>Qian Li, Stanford University</li>
        <li>Sam Ainsworth, University of Edinburgh</li>
        <li>Sami Alabed, University of Cambridge</li>
        <li>Shay Vargaftik, Vmware Research</li>
        <li>Stefano Cereda, Politecnico di Milano</li>
        <li>Taiyi Wang, University of Cambridge</li>
        <li>Thaleia Dimitra Doudali, IMDEA</li>
        <li>Valentin Radu, University of Sheffield</li>
        <li>Veljko Pejovic, University of Ljubljana</li>
        <li>Xupeng Miao, Peking University </li>
        <li>Yaniv Ben-Itzhak, Vmware Research</li>
        <li>Zheng Wang, University of Leeds</li>
        <li>Zhihao Jia, CMU</li>
    </ul>
</p>


<h4>Web Chair</h4>
<ul class="list">
    <li>Alexis Duque, Net AI</li>
</ul>

          </section>
        
          <section class="contact section" id="contact">
            <h2 class="section-title">Contact</h2>
<div>
  <p> For any question(s) related to EuroMLSys 2023, please contact the TPC Chairs <a href="mailto:eiko.yoneki@cl.cam.ac.uk">Eiko Yoneki</a> and <a href="mailto:Luigi.Nardi@cs.lth.se">Luigi Nardi</a>.
  <p><img src="/img/twitter.png" height="50px">Follow us on Twitter: <a href="https://twitter.com/euromlsys">@euromlsys</a>

</div>

          </section>
        

        <footer class="footer">
          <p>Sponsored by
            <a href="https://research.facebook.com/" class="sponsor" itemprop="url" target="_blank">
              <img src="img/meta_logo.png" height="60px" alt="Meta Research" class="photo" itemprop="image">
            </a>
            <a href="https://www.acm.org/" class="sponsor" itemprop="url" target="_blank">
              <img src="img/acm_logo.png" height="60px" alt="ACM" class="photo" itemprop="image">
            </a>
            <a href="https://www.tolacapital.com/" class="sponsor" itemprop="url" target="_blank">
              <img src="img/tola.png" height="15px" alt="Tola Capital" class="photo" itemprop="image">
            </a>
          </p>
          <p>Made with ♥ by EuroMLSys'23 team :).</p>
        </footer>
      </div>
    </div>
  </div>-->ent.write('<script src="/js/jquery.js"><\/script>')</script>
  
  
</body>
</html>
