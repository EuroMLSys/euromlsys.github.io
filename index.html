<!doctype html>
<html itemscope itemtype="http://schema.org/Event">
<head>
	<meta name="generator" content="Hugo 0.124.1">
  <title itemprop="name">The 5th Workshop on Machine Learning and Systems (EuroMLSys)</title>

  <meta charset="utf-8">
  <meta name="author" content="The 5th Workshop on Machine Learning and Systems (EuroMLSys)" />
  <meta name="description" content="The 5th Workshop on Machine Learning and Systems (EuroMLSys)">
  <meta name="viewport" content="width=device-width">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <meta property="og:title" content="The 5th Workshop on Machine Learning and Systems (EuroMLSys)" />
<meta property="og:description" content="The 5th Workshop on Machine Learning and Systems (EuroMLSys)" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://euromlsys.github.io/" /><meta property="og:image" content="https://euromlsys.github.io/img/euromlsys.jpg" />

  <meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://euromlsys.github.io/img/euromlsys.jpg" /><meta name="twitter:title" content="The 5th Workshop on Machine Learning and Systems (EuroMLSys)"/>
<meta name="twitter:description" content="The 5th Workshop on Machine Learning and Systems (EuroMLSys)"/>


  <link rel="shortcut icon" href="/img/favicon.ico" type="image/x-icon">
  <link rel="apple-touch-icon" href="/img/apple-touch-icon.png">
  <link rel="stylesheet" type="text/css" href="/css/main.css">
  <link rel="stylesheet" type="text/css" href="/css/euromlsys.css">

  <script
  src="https://code.jquery.com/jquery-1.12.4.js"
  integrity="sha256-Qw82+bXyGq6MydymqBxNPYTaUXXq7c8v3CwiYwLLNXU="
  crossorigin="anonymous"></script>
  <script type="text/javascript" src="/js/sections.js"></script>

</head>
<body>
  <div class="global">

    <nav id="nav">
  <ul class="wrapper">
    
      <li class="nav-item">
        <a href="#about" id="link-about" title="Home" class="nav-link">
          Home
        </a>
      </li>
    
      <li class="nav-item">
        <a href="#cfp" id="link-cfp" title="Call for Papers" class="nav-link">
          Call for Papers
        </a>
      </li>
    
      <li class="nav-item">
        <a href="#accepted-papers" id="link-accepted-papers" title="Accepted Papers" class="nav-link">
          Accepted Papers
        </a>
      </li>
    
      <li class="nav-item">
        <a href="#schedule" id="link-schedule" title="Program" class="nav-link">
          Program
        </a>
      </li>
    
      <li class="nav-item">
        <a href="#submission" id="link-submission" title="Submission" class="nav-link">
          Submission
        </a>
      </li>
    
      <li class="nav-item">
        <a href="#speakers" id="link-speakers" title="Keynote" class="nav-link">
          Keynote
        </a>
      </li>
    
      <li class="nav-item">
        <a href="#sponsors" id="link-sponsors" title="Sponsors" class="nav-link">
          Sponsors
        </a>
      </li>
    
      <li class="nav-item">
        <a href="#committees" id="link-committees" title="Committees" class="nav-link">
          Committees
        </a>
      </li>
    
      <li class="nav-item">
        <a href="#contact" id="link-contact" title="Contact" class="nav-link">
          Contact
        </a>
      </li>
    
  </ul>
</nav>

<hr>

    

<header class="header">
  <div class="wrapper">
    <h1 class="logo-name">
      <a class="logo-link" href="#" title="The 5th Workshop on Machine Learning and Systems (EuroMLSys)" itemprop="name">The 5th Workshop on Machine Learning and Systems (EuroMLSys)</a>
    </h1>
    <h2 class="subtitle">co-located with <a href="https://2025.eurosys.org">EuroSys '25</a></h2>
    <h2 class="tagline">March 31st 2025, Rotterdam, The Netherlands</h2>

    <div class="call-action-area">
      

      
    </div>

    <div>
      <img src="/img/euromlsys-white.png">
    </div>
  </div>
</header>

<hr>


    <div class="content" id="content">
      <div class="wrapper">
        
          <section class="about section" id="about">
            <p itemprop="description">
    The recent wave of research focusing on machine intelligence (machine learning and artificial intelligence) and its
    applications has been fuelled by both hardware improvements and deep learning frameworks that simplify the design
    and training of neural models. Advances in AI also accelerate research towards Reinforcement Learning (RL), where
    dynamic control mechanisms are designed to tackle complex tasks. Further, machine learning based optimisation, such
    as Bayesian Optimisation, is gaining traction in the computer systems community where optimisation needs to scale
    with complex and large parameter spaces; areas of interest range from hyperparameter tuning to system configuration
    tuning,
</p>
<p itemprop="description">
    The EuroMLSys workshop will provide a platform for discussing emerging trends in building frameworks,
    programming models, optimisation algorithms, and software engineering to support AI/ML applications. At
    the same time, using ML for building such frameworks or optimisation tools will be discussed. Recent
    emergence of LLM is remarked by their substantial computational requirements and optimisation in every
    possible part of the system will be important. EuroMLSys aims to bridge the gap between AI research
    and practice, through a technical program of fresh ideas on software infrastructure, tools, design
    principles, and theory/algorithms, from a systems perspective. We will also explore potential applications
    that will take advantages of ML.
</p>
<h4>News</h4>
<p>
  <ul class="list news">
    <li><b>The program is out!</b></li>


      
  </ul>
</p>

<h4>Key dates</h4>
<p>
<ul class="list">
    <li><b>Paper submission deadline:</b><s> February 7, 2025 (23:59 AoE)</s> February 11, 2025 (23:59 AoE)</li>
    <li><b>Acceptance notification:</b><s> February 21, 2025</s> February 24, 2025</li>
    <li><b>Final paper due:</b><s>  March 7, 2025</s> March 11, 2025</li>
    <li><b>Registration due:</b> March 17, 2025</li>
    <li><b>Workshop:</b> March 31, 2025 (full-day workshop)</li>
</ul>
</p>
</p>


<h4>Past Editions</h4>
<p>
<ul class="list">
    <li><a href="https://2021.euromlsys.eu">EuroMLSys 2021</a></li>
    <li><a href="https://2022.euromlsys.eu">EuroMLSys 2022</a></li>
    <li><a href="https://2023.euromlsys.eu">EuroMLSys 2023</a></li>
    <li><a href="https://2024.euromlsys.eu">EuroMLSys 2024</a></li>
</ul>
</p>
</p>

          </section>
        
          <section class="cfp section" id="cfp">
            <h2 class="section-title">Call for Papers</h2>



<p>
    A growing area of interest in machine intelligence is at the intersection of AI/ML and systems design. At
    the same time, applications of ML are growing in complexity and so is the volume of data they
    produce/consume. For computer systems to scale, new learning approaches and advanced optimisation
    techniques are needed. We also need to understand better the current AI/ML frameworks, in terms of
    their functionality, limitations, and target applications. This will clarify potential desired functions and
    future architectures. Novel machine learning methods to optimise and accelerate software and hardware
    systems must also be developed.
</p>
<p>EuroMLSys is an interdisciplinary workshop that brings together researchers in computer architecture, systems and machine learning, along with practitioners who are active in these emerging areas.
</p>
<p>Topics of interest include, but are not limited to, the following:</p>
    <ul class="list">
        <li>Scheduling algorithms for data processing clusters</li>
        <li>Custom hardware for machine learning</li>
        <li>Programming languages for machine learning</li>
        <li>Benchmarking systems (for machine learning algorithms)</li>
        <li>Synthetic input data generation for training</li>
        <li> Systems for training and serving machine learning models at scale</li>
        <li>Graph neural networks</li>
        <li>Neural network compression and pruning in systems</li>
        <li> Systems for incremental learning algorithms</li>
        <li> Large scale distributed learning algorithms in practice</li>
        <li>Database systems for large scale learning</li>
        <li>Model understanding tools (debugging, visualisation, etc.)</li>
        <li>Systems for model-free and model-based Reinforcement Learning
        </li>
        <li>Optimisation in end-to-end deep learning</li>
        <li> System optimisation using Bayesian Optimisation</li>
        <li> Acceleration of model building (e.g.,
            imitation learning in RL)</li>
        <li> Use of probabilistic models in ML/AI
            application</li>
        <li> Learning models for inferring
            network attacks,
            device/service fingerprinting,
            congestion, etc.</li>
        <li> Techniques to collect and
            analyze network data in a
            privacy-preserving manner</li>
        <li> Learning models to capture
            network events and
            control actions</li>
        <li>Machine learning in
            networking (e.g., use of
            Deep RL in networking)</li>
        <li>Analysis of distributed ML algorithms</li>
        <li>Semantics for distributed ML languages</li>
        <li>Probabilistic modelling for distributed ML algorithms</li>
        <li>Synchronisation and state control of distributed ML algorithms</li>
        <li>ML Compiler Optimisation</li>
        <li>Optimisation in Large Language Model (LLM)</li>
        <li>Novel approaches to identify and mitigate bias in ML systems</li>
        <li>Enhancing transparency and interpretability for fair AI</li>
        <li>ML systems promoting equity, fairness, and diversity</li>
        <li>Examining the societal and ecological impacts of ML systems</li>
    </ul>
    <p></p>
<p>Accepted papers will be published in the ACM Digital Library (you can opt out from this).</p>

          </section>
        
          <section class="accepted-papers section" id="accepted-papers">
            
<h4>Accepted Papers</h4>
<p>
<ul class="list">
  <li>
      <b>Machine Learning-based Deep Packet Inspection at Line Rate for RDMA on FPGAs</b> —
      <i>Maximilian Jakob Heer, Benjamin Ramhorst, Gustavo Alonso (ETH Zurich)</i>
  </li>
  <br>
  <li>
      <b>Practical Federated Learning without a Server</b> —
      <i>Akash Dhasade, Anne-Marie Kermarrec (EPFL); Erick Lavoie (University of Basel); Johan Pouwelse (Delft University of Technology); Rishi Sharma, Martijn de Vos (EPFL)</i>
  </li>
  <br>
  <li>
      <b>Leveraging Approximate Caching for Faster Retrieval-Augmented Generation</b> —
      <i>Shai Aviram Bergman, Zhang Ji (Huawei); Anne-Marie Kermarrec, Diana Petrescu, Rafael Pires, Mathis Randl, Martijn de Vos (EPFL)</i>
  </li>
  <br>
  <li>
      <b>Efficient Federated Search for Retrieval-Augmented Generation</b> —
      <i>Rachid Guerraoui, Anne-Marie Kermarrec, Diana Petrescu, Rafael Pires, Mathis Randl, Martijn de Vos (EPFL)</i>
  </li>
  <br>
  <li>
      <b>Verifying Semantic Equivalence of Large Models with Equality Saturation</b> —
      <i>Kahfi S. Zulkifli (University of Virginia); Wenbo Qian (Northeastern University); Shaowei Zhu, Yuan Zhou, Zhen Zhang (Amazon Web Services); Chang Lou (University of Virginia)</i>
  </li>
  <br>
  <li>
      <b>NeuraLUT-Assemble: Hardware-aware Assembling of Sub-Neural Networks for Efficient LUT Inference</b> —
      <i>Marta Andronic (Imperial College London); George A. Constantinides (Imperial College London, UK)</i>
  </li>
  <br>
  <li>
      <b>Systems Opportunities for LLM Fine-Tuning using Reinforcement Learning</b> —
      <i>Pedro F. Silvestre, Peter Pietzuch (Imperial College London)</i>
  </li>
  <br>
  <li>
      <b>Decentralized Adaptive Ranking using Transformers</b> —
      <i>Marcel Gregoriadis, Quinten Stokkink, Johan Pouwelse (Delft University of Technology)</i>
  </li>
  <br>
  <li>
      <b>Performance Aware LLM Load Balancer for Mixed Workloads</b> —
      <i>Kunal Jain (Microsoft); Anjaly Parayil (Microsoft Research); Ankur Mallick, Esha Choukse (Microsoft); Xiaoting Qin, Jue Zhang (Microsoft Research); Íñigo Goiri, Rujia Wang, Chetan Bansal (Microsoft); Victor Rühle (Microsoft Research); Anoop Kulkarni, Steve Kofsky (Microsoft); Saravan Rajmohan (Microsoft 365)</i>
  </li>
  <br>
  <li>
      <b>Exploiting Unstructured Sparsity in Fully Homomorphic Encrypted DNNs</b> —
      <i>Aidan Ferguson, Perry Gibson, Lara D'Agata (University of Glasgow); Parker McLeod, Ferhat Yaman, Amitabh Das, Ian Colbert (AMD); José Cano (University of Glasgow)</i>
  </li>
  <br>
  <li>
      <b>Decoupling Structural and Quantitative Knowledge in ReLU-based Deep Neural Networks</b> —
      <i>José Duato (Qsimov Quantum Computing S.L.); Jose I. Mestre, Manuel F. Dolz (Universitat Jaume I); Enrique S. Quintana-Orti (Universitat Politècnica de València); José Cano (University of Glasgow)</i>
  </li>
  <br>
  <li>
      <b>RMAI: Rethinking Memory for AI (Inference)</b> —
      <i>Amir Noohi (University of Edinburgh); Mostafa Derispour (Isfahan University of Technology); Antonio Barbalace (The University of Edinburgh)</i>
  </li>
  <br>
  <li>
      <b>Understanding Oversubscribed Memory Management for Deep Learning Training</b> —
      <i>Mao Lin, Hyeran Jeon (University of California, Merced)</i>
  </li>
  <br>
  <li>
      <b>Priority-Aware Preemptive Scheduling for Mixed-Priority Workloads in MoE Inference</b> —
      <i>Mohammad Siavashi (KTH Royal Institute of Technology); Faezeh Keshmiri Dindarloo (Unaffiliated); Dejan Kostic, Marco Chiesa (KTH Royal Institute of Technology)</i>
  </li>
  <br>
  <li>
      <b>Diagnosing and Resolving Cloud Platform Instability with Multi-modal RAG LLMs</b> —
      <i>Yifan Wang, Kenneth P. Birman (Cornell University)</i>
  </li>
  <br>
  <li>
      <b>FlexInfer: Breaking Memory Constraint via Flexible and Efficient Offloading for On-Device LLM Inference</b> —
      <i>Hongchao Du, Shangyu Wu (City University of Hong Kong); Arina Kharlamova (Mohamed bin Zayed University of Artificial Intelligence); Nan Guan (City University of Hong Kong); Chun Jason Xue (Mohamed bin Zayed University of Artificial Intelligence)</i>
  </li>
  <br>
  <li>
      <b>Deferred prefill for throughput maximization in LLM inference</b> —
      <i>Moonmoon Mohanty, Gautham Bolar, Preetam Patil (Indian Institute of Science Bangalore); UmaMaheswari Devi, Felix George, Pratibha Moogi (IBM Research - India); Parimal Parag (Indian Institute of Science Bangalore)</i>
  </li>
  <br>
  <li>
      <b>AMPLE: Event-Driven Accelerator for Mixed-Precision Inference of Graph Neural Networks</b> —
      <i>Pedro Gimenes (Imperial College London, UK); Aaron Zhao (Imperial College London); George A. Constantinides (Imperial College London, UK)</i>
  </li>
  <br>
  <li>
      <b>Client Availability in Federated Learning: It Matters!</b> —
      <i>Dhruv Garg, Debopam Sanyal (Georgia Institute of Technology); Myungjin Lee (Cisco Systems); Alexey Tumanov, Ada Gavrilovska (Georgia Institute of Technology)</i>
  </li>
  <p>
  <h4>Accepted Poster Papers</h4>
  <p>
  <li>
      <b>Global-QSGD: Allreduce-Compatible Quantization for Distributed Learning with Theoretical Guarantees</b> —
      <i>Jihao Xin, Marco Canini, Peter Richtárik (KAUST); Samuel Horváth (MBZUAI)</i>
  </li>
  <br>
  <li>
      <b>Hybrid Task Scheduling for Optimized Neural Network Inference on Skin Lesions in Resource-Constrained Systems</b> —
      <i>Diogen Babuc, Teodor-Florin Fortiş (West University of Timişoara)</i>
  </li>
  <br>
  <li>
      <b>Cross-Domain DRL Agents for Efficient Job Placement in the Cloud-Edge Continuum</b> —
      <i>Theodoros Aslanidis (University College Dublin); Sokol Kosta (Department of Electronic Systems, Aalborg University Copenhagen); Spyros Lalis (University of Thessaly); Dimitris Chatzopoulos (University College Dublin)</i>
  </li>
  <br>
  <li>
      <b>Towards a Unified Framework for Split Learning</b> —
      <i>Boris Radovič (KAUST & University of Ljubljana); Marco Canini (KAUST); Samuel Horváth (MBZUAI); Veljko Pejović (University of Ljubljana); Praneeth Vepakomma (MBZUAI & MIT)</i>
  </li>
  <br>
  <li>
      <b>Manage the Workloads not the Cluster: Designing a Control Plane for Large-Scale AI Clusters</b> —
      <i>Ruiqi Lai, Siyu Cao, Leqi Li (NTU Singapore); Luo Mai (University of Edinburgh); Dmitrii Ustiugov (NTU Singapore)</i>
  </li>
  <br>
  <li>
      <b>Harnessing Increased Client Participation with Cohort-Parallel Federated Learning</b> —
      <i>Akash Dhasade, Anne-Marie Kermarrec (EPFL); Tuan-Ahn Nguyen (Independent Researcher); Rafael Pires, Martijn de Vos (EPFL)</i>
  </li>
  <br>
  <li>
      <b>Accelerating MoE Model Inference with Expert Sharding</b> —
      <i>Oana Balmau (McGill); Anne-Marie Kermarrec, Rafael Pires, André Loureiro Espírito Santo, Martijn de Vos, Milos Vujasinovic (EPFL)</i>
  </li>
  <br>
  <li>
      <b>TAGC: Optimizing Gradient Communication in Distributed Transformer Training</b> —
      <i>Igor Polyakov (VK, ITMO University); Alexey Dukhanov (ITMO University); Egor Spirin (VK Lab)</i>
  </li>
  <br>
  <li>
      <b>β-GNN: A Robust Ensemble Approach Against Graph Structure Perturbation</b> —
      <i>Haci Ismail Aslan (Technical University of Berlin); Philipp Wiesner, Ping Xiong, Odej Kao (Technische Universität Berlin)</i>
  </li>
  <br>
  <li>
      <b>May the Memory Be With You: Efficient and Infinitely Updatable State for Large Language Models</b> —
      <i>Excel Chukwu, Laurent Bindschaedler (Max Planck Institute for Software Systems)</i>
  </li>
  <br>
  <li>
      <b>Towards Asynchronous Peer-to-Peer Federated Learning for Heterogeneous Systems</b> —
      <i>Christos Sad (Aristotle University of Thessaloniki); George Retsinas, Dimitrios Soudris (National Technical University of Athens); Kostas Siozios (Aristotle University of Thessaloniki); Dimosthenis Masouros (National Technical University of Athens)</i>
  </li>
  <br>
  <li>
      <b>Beyond Test-Time Compute Strategies: Advocating Energy-per-Token in LLM Inference</b> —
      <i>Patrick Wilhelm, Thorsten Wittkopp, Odej Kao (Technische Universität Berlin)</i>
  </li>
  <br>
  <li>
      <b>Utilizing Large Language Models for Ablation Studies in Machine Learning and Deep Learning</b> —
      <i>Sina Sheikholeslami, Hamid Ghasemirahni, Amir H. Payberah, Tianze Wang (KTH Royal Institute of Technology); Jim Dowling (Hopsworks AB); Vladimir Vlassov (KTH Royal Institute of Techonology, Sweden)</i>
  </li>
  <br>
  <li>
      <b>Rethinking Observability for AI workloads on Multi-tenant public clouds</b> —
      <i>Theophilus A. Benson (Carnegie Mellon University)</i>
  </li>
  <br>
  <li>
      <b>OptimusNIC: Offloading Optimizer State to SmartNICs for Efficient Large-Scale AI Training</b> —
      <i>Achref Rebai, Marco Canini (KAUST)</i>
  </li>
  <br>
  <li>
      <b>Analysis of Information Propagation in Ethereum Network Using Combined Graph Attention Network and Reinforcement Learning to Optimize Network Efficiency and Scalability</b> —
      <i>Stefan Behfar, Richard Mortier, Jon Crowcroft (University of Cambridge)</i>
  </li> 
          </section>
        
          <section class="schedule section" id="schedule">
            <h2 class="section-title">Program</h2>
<p>The workshop program is available <a target="_blank" rel="noopener noreferrer" href="pdf/Program_2025_final.pdf"> here</a></p>



          </section>
        
          <section class="submission section" id="submission">
            <h2 class="section-title">Submission</h2>

<p>Papers must be submitted electronically as PDF files, formatted for 8.5x11-inch paper. 
        Submissions will be up to 6 pages long, including figures, and tables, with 10-point font, in a two-column
        format. Bibliographic references are not included in the 6-page limit. Submitted papers must use the official <a
        href="https://www.acm.org/publications/authors/submissions">SIGPLAN Latex / MS Word templates.</a></p>
<p> Submissions will be single-blind.</p>



<p><b>Submit your paper at:</b> <a href="https://euromlsys25.hotcrp.com/paper/new">https://euromlsys25.hotcrp.com/paper/new</a></p>

          </section>
        
          <section class="speakers section" id="speakers">
            <h2 class="section-title">Keynote</h2>
<p> </p>
<ul class="speakers-list">

  <li class="speakers-item" itemprop="performer" itemscope itemtype="http://schema.org/Person">
    
      <span class="speaker-photo">
        <img class="photo" src="/img/zhihao.jpg" alt="Zhihao Jia" itemprop="image">
      </span>
    

    <h3 class="speech-title">
      
      <span class="speech-time">14:00</span>
      
      <span>Zhihao Jia</span>
      <span class="speakers-company">Assistant Professor in the Computer Science Department at Carnegie Mellon University</span>
    </h3>

    <h3 class="speakers-name">Superoptimizing Machine Learning Systems
      
    </h3>
    

    <p class="schedule-description"><p>The success of machine learning (ML) depends heavily on ML systems, which enable rapid prototyping and efficient deployment of ML models across diverse hardware platforms. However, the increasing parallelism and specialization of ML hardware—driven by demands for greater performance within fixed power constraints—make traditional approaches to building ML systems, which rely heavily on manual design and implementation of systems optimizations, increasingly impractical.</p>
<p>In this talk, I will present our research pursuing a new methodology that enables ML systems and compilers to automatically generate, verify, and deploy model- and hardware-specific optimizations with minimal human effort. First, I will introduce a multi-level superoptimizer that automatically translates pure mathematical definitions of ML models into highly optimized GPU code. Next, I will showcase a system that accelerates distributed ML training and serving by automatically discovering fast parallelization strategies tailored to specific hardware platforms. Our work is actively deployed by industrial companies, national laboratories, and academic groups, collectively receiving millions of downloads.</p>
<p>Finally, I will outline future research directions aimed at developing next-generation ML systems, including new abstractions and software stacks to address the growing parallelism, specialization, and heterogeneity of emerging ML hardware.&quot;</p>
</p>
    <p class="speakers-bio">Bio:  Zhihao Jia is an Assistant Professor in the Computer Science Department at Carnegie Mellon University. His research interests lie in the intersection of systems and machine learning, with a focus on building efficient, scalable, and affordable systems for ML applications. He received his PhD from Stanford University, where he earned the Arthur Samuel Best Doctoral Thesis Award. His research has been recognized with a Sloan fellowship, an R&amp;D 100 award, paper awards from ASPLOS and ACL, and research awards from Amazon, Cisco, Google, Meta, NVIDIA, Oracle, Qualcomm, and Samsung.</p>

  </li>

</ul>

          </section>
        
          <section class="sponsors section" id="sponsors">
            <h2 class="section-title">Sponsors</h2>
<br>
<ul class="sponsors-list">

  <li class="sponsor-item" itemscope itemtype="http://schema.org/Organization">
    <a href="https://www.acm.org/" class="sponsor--link" itemprop="url" target="_blank">
      <img src="img/acm_logo.png" alt="ACM" class="photo" itemprop="image">
    </a>
  </li>

  <li class="sponsor-item" itemscope itemtype="http://schema.org/Organization">
    <a href="https://www.tolacapital.com/" class="sponsor--link" itemprop="url" target="_blank">
      <img src="img/tola.png" alt="Tola Capital" class="photo" itemprop="image">
    </a>
  </li>

</ul>


          </section>
        
          <section class="committees section" id="committees">
            <h2 class="section-title">Committees</h2>

<h4>Workshop and TPC Chairs</h4>
<p>
<ul class="list">
    <li>Eiko Yoneki, University of Cambridge, <a href="https://www.cl.cam.ac.uk/~ey204/">https://www.cl.cam.ac.uk/~ey204/</a></li>
    <li>Amir H. Payberah, KTH, <a href="https://payberah.github.io/">https://payberah.github.io/</a></li>
</ul>
</p>
<p>
<h4>Technical Program Committee</h4>
<p>
    <ul class="list">
        <li>Aaron Zhao, Imperial College London</li>
        <li>Andy Twigg, Google</li>
        <li>Ahmed Sayed, Queen Mary University of London</li>
        <li>Alexandros Koliousis, Northeastern University London and Institute for Experiential AI</li>
        <li>Amitabha Roy, Google</li>
        <li>Chi Zhang, Brandeis University</li>
        <li>Christos Bouganis, Imperial College London</li>
        <li>Chunwei Xia , University of Leeds</li>
        <li>Daniel Goodman, Oracle </li>
        <li>Daniel Mendoza, Stanford University</li>
        <li>Dawei Li, Amazon</li>
        <li>Deepak George Thomas, Iowa State University </li>
        <li>Dimitris Chatzopoulos, University College Dublin</li>
        <li>Fiodar Kazhamiaka,Stanford University</li>
        <li>Guilherme H. Apostolo, Vrije Universiteit Amsterdam</li>        
        <li>Guoliang He, University of Cambridge</li>        
        <li>Jenny Huang, Nvidia</li>
        <li>Joana Tirana, University College Dublin</li>
        <li>Jon Crowcroft, University of Cambridge</li>
        <li>Jose Cano, University of Glasgow</li>
        <li>Luo Mai, University of Edinburgh</li>
        <li>Mark Zhao, Stanford University</li>
        <li>Mengying Zhou, Fudan University</li>
        <li>Nikolas Ioannou, Google</li>
        <li>Paul Patras, University of Edinburgh</li>
        <li>Peter Pietzuch, Imperial College London</li>
        <li>Peter Triantafillou, University of Warwick</li>
        <li>Pinar Tözün, IT University of Copenhagen</li>
        <li>Pouya Hamadanian, MIT</li>
        <li>Sam Ainsworth, University of Edinburgh</li>
        <li>Sami Alabed, Deepmind</li>
        <li>Sandra Siby, NYU Abu Dhabi</li>
        <li>Shivaram Venkataraman, University of Wisconsin-Madison</li>
        <li>Taiyi Wang, University of Cambridge</li>
        <li>Thaleia Dimitra Doudali, IMDEA</li>
        <li>Valentin Radu, University of Sheffield</li>
        <li>Veljko Pejovic, University of Ljubljana</li>
        <li>Xupeng Miao, Peking University </li>
        <li>Yaniv Ben-Itzhak, Broadcom</li>
        <li>Youhe Jiang, University of Cambridge</li>
        <li>Yuchen Zhao, University of York</li>
        <li>Zak Singh, University of Cambridge</li>
        <li>Zheng Wang, University of Leeds</li>
        <li>Zhihao Jia, CMU</li>
    </ul>
</p>



<h4>Web Chair</h4>
<ul class="list">
    <li>Alexis Duque, Net AI</li>
</ul>

          </section>
        
          <section class="contact section" id="contact">
            <h2 class="section-title">Contact</h2>
<div>
  <p> For any question(s) related to EuroMLSys 2025, please contact the TPC Chairs <a href="mailto:eiko.yoneki@cl.cam.ac.uk">Eiko Yoneki</a> and <a href="mailto:payberah@kth.se">Amir H. Payberah</a>.
  <p><img src="/img/twitter.png" height="50px">Follow us on Twitter: <a href="https://twitter.com/euromlsys">@euromlsys</a>

</div>

          </section>
        

        <footer class="footer">
          <p>Sponsored by
            
            <a href="https://www.acm.org/" class="sponsor" itemprop="url" target="_blank">
              <img src="img/acm_logo.png" height="60px" alt="ACM" class="photo" itemprop="image">
            </a>
            <a href="https://www.tolacapital.com/" class="sponsor" itemprop="url" target="_blank">
              <img src="img/tola.png" height="15px" alt="Tola Capital" class="photo" itemprop="image">
            </a>
          </p>
          <p>Made with ♥ by EuroMLSys'25 team :).</p>
        </footer>
      </div>
    </div>
  
</body>
</html>
